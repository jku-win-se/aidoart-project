Title,Abstract,Author Keywords
A Data Quality-Driven View of MLOps,"Developing machine learning models can be seen as a process similar to the one established for traditional software development. A key difference between the two lies in the strong dependency between the quality of a machine learning model and the quality of the data used to train or perform evaluations. In this work, we demonstrate how different aspects of data quality propagate through various stages of machine learning development. By performing a joint analysis of the impact of well-known data quality dimensions and the downstream machine learning process, we show that different components of a typical MLOps pipeline can be efficiently designed, providing both a technical and theoretical perspective.",None
A Framework for Continuous Regression and Integration Testing in IoT Systems Based on Deep Learning and Search-Based Techniques,"Tremendous systems are rapidly evolving based on the trendy Internet of Things (IoT) in various domains. Different technologies are used for communication between the massive connected devices through all layers of the IoT system, causing many security and performance issues. Regression and integration testing are considered repeatedly, in which the vast costs and efforts associated with the frequent execution of these inflated test suites hinder the adequate testing of such systems. This necessitates the focus on exploring innovative scalable testing approaches for large test suites in IoT-based systems. In this paper, a scalable framework for continuous integration and regression testing in IoT-based systems (IoT-CIRTF) is proposed, based on IoT-related criteria for test case prioritization and selection. The framework utilizes search-based techniques to provide an optimized prioritized set of test cases to select from. The selection is based on a trained prediction model for IoT standard components using supervised deep learning algorithms to continuously ensure the overall reliability of IoT-based systems. The experiments are held on two GSM datasets. The experimental results achieved prioritization accuracy up to 90% and 92% for regression testing and integration testing respectively. This provides an enhanced and efficient framework for continuous testing of IoT-based systems, as per IoT-related criteria for the prioritization and selection purposes.","Deep learning, integration testing, IoT, regression testing, search-based techniques, test case prioritization, test case selection"
A Hitchhiker's Guide to Model-Driven Engineering for Data-Centric Systems,The models and data framework demystifies the different roles that models and data play in software development and operation and clarifies where machine learning and artificial intelligence techniques could be used.,
A learning algorithm for optimizing continuous integration development and testing practice,"Continuous integration, at its core, includes a set of practices that aim to prevent and reduce the cost of software integration issues by merging working software copies often. Regression testing is considered a good practice in software development with continuous integration, which ensures that code changes are not negatively affecting software functionality. As, nowadays, software development is carried out iteratively, with small code increments continuously developed and regression tested, it is of critical importance that continuous regression testing is time efficient. However, in practice, regression testing is often long lasting and faces scalability problems as software grows larger or as software changes are made more frequently. One contributing factor to these issues is test redundancy, which causes the same software functionality being tested multiple times across a test suite. In large-scale software, especially highly configurable software, redundancy in continuous regression testing can significantly grow the size of test suites and negatively affect the cost effectiveness of continuous integration. This paper presents a practical learning algorithm for optimizing continuous integration testing by reducing ineffective test redundancy in regression suites. The novelty of the algorithm lies in learning and predicting the fault-detection effectiveness of continuous integration tests using historical test records and combining this information with coverage-based redundancy metrics. The goal is to identify ineffective redundancy, which is maximally reduced in the resulting regression test suite, thus reducing test time and improving the performance of continuous integration. We apply and evaluate the algorithm in two industrial projects of continuous integration. The results show that the proposed algorithm can improve the efficiency of continuous integration practice in terms of decreasing test execution time by 38% on average compared to the industry practice of our case study and by 40% on average compared to the retest-all approach. The results further demonstrate no significant reduction in fault-detection effectiveness of continuous regression testing. This suggests that the proposed algorithm contributes to the state of the practice in the continuous integration development and testing of highly configurable systems.","continuous integration, continuous integration testing, highly configurable software, highly interleaved test predictive algorithm, regression testing, regression trees, test optimization, test redundancy"
A learning-based framework for engineering feature-oriented self-adaptive software systems,"Self-adaptive software systems are capable of adjusting their behavior at runtime to achieve certain functional or quality-of-service goals. Often a representation that reflects the internal structure of the managed system is used to reason about its characteristics and make the appropriate adaptation decisions. However, runtime conditions can radically change the internal structure in ways that were not accounted for during their design. As a result, unanticipated changes at runtime that violate the assumptions made about the internal structure of the system could degrade the accuracy of the adaptation decisions. We present an approach for engineering self-adaptive software systems that brings about two innovations: 1) a feature-oriented approach for representing engineers' knowledge of adaptation choices that are deemed practical, and 2) an online learning-based approach for assessing and reasoning about adaptation decisions that does not require an explicit representation of the internal structure of the managed software system. Engineers' knowledge, represented in feature-models, adds structure to learning, which in turn makes online learning feasible. We present an empirical evaluation of the framework using a real-world self-adaptive software system. Results demonstrate the framework's ability to accurately learn the changing dynamics of the system while achieving efficient analysis and adaptation. © 1976-2012 IEEE.","Autonomic computing, Feature-orientation, Machine learning, Self-adaptive software"
A microservice-based framework for multi-level testing of cyber-physical systems,"In the last years, the use of microservice architectures is spreading in Cyber-Physical Systems (CPSs) and Internet of Things (IoT) domains. CPSs are systems that integrate digital cyber computations with physical processes. The development of software for CPSs demands a constant maintenance to support new requirements, bug fixes, and deal with hardware obsolescence. The key in this process is code testing and more if the code is fragmented during the development of CPSs. It is important to remark that this process is challenging and time-consuming. In this paper, we report on the experience of instantiating of the microservice-based architecture for DevOps of CPSs to test elevator dispatching algorithms across different test levels (i.e., SiL, HiL and Operation). Such an architecture allows for a continuous deployment, monitoring and validation of CPSs. By integrating the approach with a real industrial case study, we demonstrate that our approach reduces significantly the time needed in the testing process and consequently, reduces the economic cost of the entire process.","Cyber-Physical Systems, Elevators, Testing"
A model-driven approach for continuous performance engineering in microservice-based systems,"The Author(s)Microservices are quite widely impacting on the software industry in recent years. Rapid evolution and continuous deployment represent specific benefits of microservice-based systems, but they may have a significant impact on non-functional properties like performance. Despite the obvious relevance of this property, there is still a lack of systematic approaches that explicitly take into account performance issues in the lifecycle of microservice-based systems. In such a context of evolution and re-deployment, Model-Driven Engineering techniques can provide major support to various software engineering activities, and in particular they can allow managing the relationships between a running system and its architectural model. In this paper, we propose a model-driven integrated approach that exploits traceability relationships between the monitored data of a microservice-based running system and its architectural model to derive recommended refactoring actions that lead to performance improvement. The approach has been applied and validated on two microservice-based systems, in the domain of e-commerce and ticket reservation, respectively, whose architectural models have been designed in UML profiled with MARTE.","Continuous deployment, Microservices, Model-driven engineering, Performance engineering, Software evolution, Software refactoring"
A Model-Driven Approach to Continuous Delivery of Cloud Resources,"DevOps is a paradigm which brings practices and tools that optimize the software delivery time. Cloud-based DevOps processes facilitate the continuous delivery of infrastructure and software applications (i.e. cloud resources). In particular, Infrastructure as Code is the cornerstone of DevOps for automating the infrastructure provisioning based on practices from software development. There exist several Configuration Management Tools (CMTs) that use script languages to define the infrastructure provisioning to be deployed in a particular cloud provider. However, manual setting of the script languages to establish the infrastructure provisioning in a CMT for a particular cloud provider is a time-consuming and error-prone activity. For these reasons, the aim of my PhD research is proposing a model-driven approach to abstract and automate a continuous delivery process of cloud resources through model-driven techniques and DevOps. In addition, this approach seeks to cover the development process of cloud resources in development, testing and production environments.","Cloud computing, Cloud resources, Continuous delivery, DevOps, Infrastructure as code, Model-Driven development"
A model-driven approach to continuous practices for modern cloud-based web applications,"In this paper, we propose a model-driven approach to Continuous Software Integration and Deployment (CI/CD) for modern cloud-based applications. Key to our approach is a formal graphical modelling language for the specification of the processes and tasks involved. Based on these specifications the complete CI/CD configurations are generated fully and automatically guaranteeing their correctness with regard to the specification by construction. This way typical sources of critical errors can be avoided lowering the hurdle to introduce CI/CD especially in mature projects. We demonstrate the power of our model-driven approach with the help of an industrial web application - a prime example for cloud-based applications.","Cloud-based Applications, Continuous Deployment, Continuous Integration, Model-driven"
A Model-Driven Architectural Design Method for Big Data Analytics Applications,"Big data analytics (BDA) applications use machine learning to extract valuable insights from large, fast, and heterogeneous data sources. The architectural design and evaluation of BDA applications entail new challenges to integrate emerging machine learning algorithms with cutting-edge practices whilst ensuring performance levels even in the presence of large data volume, velocity, and variety (3Vs). This paper presents a design process approach based on the Attribute-Driven Design (ADD) method and Architecture tradeoff analysis method (ATAM) to specify, deploy, and monitor performance metrics in BDA applications supported by domain-specific modeling and DevOps. Our design process starts with the definition of architectural drivers, followed by functional and deployment specification through integrated high-level modeling which enables quality scenarios monitoring. We used two use cases from avionics to evaluate this proposal, and the preliminary results suggest advantages by integrating multiple views, automating deployment and monitoring compared to similar approaches.","ADD, ATAM, Attribute-Driven Design, Big data analytics deployment, DevOps, Domain-specific model, Quality Scenarios, Software architecture"
A model-driven engineering framework to support the functional safety process,"The design of safety-related systems traditionally has long and costly development cycles due to the highly manual safety engineering process, which is guided by industry standards. In this paper, we present a modelling framework that supports DevOps principles of continuous testing and fast development iterations for the design of safety-critical systems. We show how modelling can help introducing DevOps in the context of functional safety analysis, and we also report how DevOps was used during the development of the framework.","Automotive, Devops, Iso26262, Safety critical, Verification"
A model-driven workflow for distributed microservice development,"Model-driven Development (MDD) is a software engineering approach that abstracts a software's design leveraging models. In particular, the development of complex, service-based architectures is considered to benefit from MDD techniques like model validation, transformation, and code generation. This paper presents an MDD-based workflow for distributed, DevOps-based microservice development and identifies the involved model types. They provide the foundation for the subsequent development of modeling languages to employ MDD for MSA engineering.","Distributed microservice development, Microservice architecture, Model-driven microservice development, Modeling languages, Viewpoint modeling"
A Modular and Composable Approach to Develop Trusted Artificial Intelligence,"Trustworthy artificial intelligence (Trusted AI) is of utmost importance when learning-enabled components (LECs) are used in autonomous, safety-critical systems. When reliant on deep learning, these systems need to address the reliability, robustness, and interpretability of learning models. In addition to developing specific strategies to address each of these concerns, appropriate software architectures are needed to coordinate LECs and ensure they deliver acceptable behavior under uncertain conditions. This work proposes a model-driven framework of loosely-coupled modular services designed to monitor and control LECs with respect to Trusted AI assurance concerns. The proposed framework is composable, deploying independent services to improve the resilience and robustness of AI systems. The overarching objective of this framework is to support software engineering principles focusing on modularity, composability, and reusability in order to facilitate development and maintenance tasks, while also increasing stakeholder confidence in Trusted AI systems. To demonstrate this framework, it has been implemented to manage the operation of an autonomous rover's vision-based LEC while exposed to uncertain environmental conditions.","artificial intelligence, deep learning, models at run time, self-adaptive systems, software engineering"
A process-oriented build tool for safety-critical model-based software development,"By conquering new application areas, software complexity and size increases steadily. Development cycles must become faster to deliver critical updates in shorter time. Simultaneously, software takes over more and more safety-critical tasks, requiring strict software development processes. Up to today, these processes suffer from extensive manual review work and written, static documentation in form of standards, checklists, and procedures. This paper presents a monolithic, process-oriented build tool for model-based development in MATLAB, Simulink, and Stateflow. Beyond classical build automation functionality, it supports and accelerates process workflows. The tool provides infrastructure to formalize and ship workflows, checklists, and standards, but also features to assess completeness, consistency, compliance, and cleanliness with respect to them. Additionally, it allows definition of dynamic, incrementally updated checklists, and composes traceability in parallel with the build. The efficacy and achievable process coverage is demonstrated in an example application.","Build Automation, Component-based Software Engineering, Continuous Integration, Model Scaffolding, Model Standards, Software Development Process, Workflow Management System"
A tag-based recommender system for regression test case prioritization,"In continuous integration development environments (CI), the software undergoes frequent changes due to bug fixes or new feature requests. Some of these changes may accidentally cause regression issues to the newly released software version. To ensure the correctness of the newly released software, it is important to perform enough testing prior to code submission to avoid breaking builds. Regression testing is one of the important maintenance activities that can control the quality and reliability of modified software, but it can also be very expensive. Test case prioritization can reduce the costs of regression testing by reordering test cases to meet testing objectives better. To date, various test prioritization techniques have been developed, however, the majority of the proposed approaches utilize static or dynamic analyses to decide which test cases should be selected. These analyses often have significant cost overhead and are time consuming. This paper introduces a new method for automatic test case prioritization in a CI environment intending to minimize the testing cost. Our proposed approach uses information retrieval to automatically select test cases based on their textual similarity to the portion of the code that has been changed. Our technique not only helps developers to organize and manage the software repository but also helps them to find the relevant resources quickly. To evaluate our approach, we performed an empirical study using 37 versions of 6 open source applications. The results of our empirical study indicate that our proposed method can improve the effectiveness and efficiency of test case prioritization technique.","Continuous Integration, IR-based Regression Testing, Recommender Systems, Regression Testing, Tag-based Recommender System, Test Case Prioritization"
A testing frameworks for mobile embedded systems using MDA,"Embedded system can give you many benefits in putting it in your device, such as mobile phones, appliances at home, machines at the bank, lottery machine and many more, just make sure it is undergoing in embedded systems testing to have the device check. You must know that putting an embedded system in any of your device (either at home or in your business) can vary be helpful in your daily life and for the near future.One of the important phases in the life cycle of embedded software development process is the designing phase. There are different models used in this particular phase including class diagrams, state diagrams and use cases etc. To test the conformance of the software it is very essential that test cases should be derived from these specific models. Similarly regressions testing through these models are very significant for testing of modified software. There are several regression testing approaches based on these model in literature. This survey report is the analysis of the model based regression testing techniques according to the parameter identified during this study. The summary as well as the analysis of the approaches is discussed in this survey report. In the end we concluded the survey by identifying the areas of further research in the field of model based regression testing.","Embedded testing, MDA, model based regression testing, Regression testing, Testing evaluation parameters, UML regression testing"
A Unified test framework for continuous integration testing of SOA solutions,"The quality of Service Oriented Architecture (SOA) solutions is becoming more and more important along with the increasing adoption of SOA. Continuous Integration Testing (CIT) is an effective technology to discover bugs as early as possible. However, the diversity of programming models used in an SOA solution and the distribution nature of an SOA solution pose new challenges for CIT. Existing testing frameworks more focus on the integration testing of applications developed by a single programming model. In this paper, a unified test framework is proposed to overcome these limitations and enable the CIT of SOA solutions across the whole development lifecycle. This framework is designed following the Model Driven Architecture (MDA). The information of an executable test case is separated into two layers: the behavior layer and the configuration layer. The behavior layer represents the test logic of a test case and is platform independent. The configuration layer contains the platform specific information and is configurable for different programming models. An extensible and pluggable test execution engine is specially designed to execute the integration test cases. A global test case identifier instrumentation approach is used to merge the distributed test case execution traces captured by ITCAM - an IBM integrated management tool. A verification approach supporting Boolean expression and back-end service interaction verification is proposed to verify the test execution result. Initial experiments have shown the effectiveness of this unified test framework. © 2009 IEEE.","Continuous integration testing, Service oriented architecture"
ACCORDANT: A domain specific-model and DevOps approach for big data analytics architectures,"Big data analytics (BDA) applications use machine learning algorithms to extract valuable insights from large, fast, and heterogeneous data sources. New software engineering challenges for BDA applications include ensuring performance levels of data-driven algorithms even in the presence of large data volume, velocity, and variety (3Vs). BDA software complexity frequently leads to delayed deployments, longer development cycles, and challenging performance assessment. This paper proposes a Domain-Specific Model (DSM), and DevOps practices to design, deploy, and monitor performance metrics in BDA applications. Our proposal includes a design process, and a framework to define architectural inputs, software components, and deployment strategies through integrated high-level abstractions to enable QS monitoring. We evaluate our approach with four use cases from different domains to demonstrate a high level of generalization. Our results show a shorter deployment and monitoring times, and a higher gain factor per iteration compared to similar approaches.","Big data analytics deployment, DevOps, Domain-specific model, Performance monitoring, Quality scenarios, Software architecture"
Active continuous quality control,"We present Active Continuous Quality Control (ACQC), a novel approach that employs incremental active automata learning technology periodically in order to infer evolving behavioral automata of complex applications accompanying the development process. This way we are able to closely monitor and steer the evolution of applications throughout their whole life-cycle with minimum manual effort. Key to this approach is to establish a stable level for comparison via an incrementally growing behavioral abstraction in terms of a user-centric communication alphabet: The letters of this alphabet, which may correspond to whole use cases, are intended to directly express the functionality from the user perspective. At the same time their choice allows one to focus on specific aspects, which establishes tailored abstraction levels on demand, which may be refined by adding new letters in the course of the systems evolution. This way ACQC does not only allow us to reveal serious bugs simply by inspecting difference views of the (tailored) models, but also to visually follow and control the effects of (intended) changes, which complements our model-checking-based quality control. All this will be illustrated along reallife scenarios that arose during the component-based development of a commercial editorial system. Copyright 2013 ACM.","Active learning, Model checking, Testing, Validation"
Adaptive application development and integration process for modern automotive software,"Due to fast progress in information technologies and long lifecycles of vehicles, there are ever-increasing expectations in modern automotive software development regarding the flexibility to integrate updates and new functions quickly into already existing systems. This paper proposes a process, that is especially suitable for the development of new functions in higher programming languages and the usage of machine learning models. When developed in a tool like MATLAB, code generators can be used to integrate the function step-by-step into a service-oriented automotive E/E-architecture. It is based on a classic V-model process and uses integration steps according to the XiL approach. The key aspect is the frontloading of verification and validation into the steps as early as possible to keep iteration cycles fast. The proposed process is applied to the development of a Neural Network Model Predictive Control (NNMPC) for a Heating, Ventilation and Air-Conditioning (HVAC) unit of a city bus. The resulting NNMPC is then integrated into a system based on the AUTOSAR adaptive platform. That allowed the function to be developed and integrated quickly and seems to be a promising approach to bring new functions into already existing automotive E/E-architectures.","automotive software, AUTOSAR Adaptive, service-oriented architecture, software development process"
Adaptive behavioral model learning for software product lines,"Behavioral models enable the analysis of the functionality of software product lines (SPL), e.g., model checking and model-based testing. Model learning aims to construct behavioral models. Due to the commonalities among the products of an SPL, it is possible to reuse the previously-learned models during the model learning process. In this paper, an adaptive approach, called PL?, for learning the product models of an SPL is presented based on the well-known L? algorithm. In this method, after learning each product, the sequences in the final observation table are stored in a repository which is used to initialize the observation table of the remaining products. The proposed algorithm is evaluated on two open-source SPLs and the learning cost is measured in terms of the number of rounds, resets, and input symbols. The results show that for complex SPLs, the total learning cost of PL? is significantly lower than that of the non-adaptive method in terms of all three metrics. Furthermore, it is observed that the order of learning products affects the efficiency of PL?. We introduce a heuristic to determine an ordering which reduces the total cost of adaptive learning.","adaptive model learning, automata learning, finite state machines, software product lines"
Adaptive Learning for Learn-Based Regression Testing,Regression testing is an important activity to prevent the introduction of regressions into software updates. Learn-based testing can be used to automatically check new versions of a system for regressions on a system level. This is done by learning a model of the system and model checking this model for system property violations. Learning the model of a large system can take an unpractical amount of time however. In this work we investigate if the concept of adaptive learning can improve the learning speed of a model in a regression testing scenario. We have performed several experiments with this technique on two systems: ToDoMVC and SSH. We find that there can be a large benefit to using adaptive learning. In addition we find three main factors that influence the benefit of adaptive learning. There are however also some shortcomings to adaptive learning that should be investigated further.,
Adaptive Reward Computation in Reinforcement Learning-Based Continuous Integration Testing,"Reinforcement learning (RL) has been applied to prioritizing test cases in Continuous Integration (CI) testing, where the reward plays a crucial role. It has been demonstrated that historical information-based reward function can improve the effectiveness of the test case prioritization (TCP). However, the inherent character of frequent iterations in CI can produce a considerable accumulation of historical information, which may decrease TCP efficiency and result in slow feedback. In this paper, the partial historical information is considered in the reward computation, where sliding window techniques are adopted to capture the possible efficient information. Firstly, the fixed-size sliding window is introduced to set a fixed length of recent historical information for each CI test. Then dynamic sliding window techniques are proposed, where the window size is continuously adaptive to each CI testing. Two methods are proposed, the test suite-based dynamic sliding window and the individual test case-based dynamic sliding window. The empirical studies are conducted on fourteen industrial-level programs, and the results reveal that under limited time, the sliding window-based reward function can effectively improve the TCP effect, where the NAPFD (Normalized Average Percentage of Faults Detected) and Recall of the dynamic sliding windows are better than that of the fixed-size sliding window. In particular, the individual test case-based dynamic sliding window approach can rank 74.18% failed test cases in the top 50% of the sorting sequence, with 1.35% improvement of NAPFD and 6.66 positions increased in TTF (Test to Fail).","Continuous integration, reinforcement learning, reward computation, sliding window, test case prioritization"
Agent-based business process orchestration for IoT,"The so-called Internet of Things is of increasing importance for facilitating productivity across industries, i.e., by connecting sensors with manufacturing lines and IT system landscapes with an increasing degree of autonomy. In this context, a common challenge is enabling reasonable trade-offs between structure and control on the one hand and flexibility and human-like intelligent behavior on the other hand. To address this challenge, we establish the need for and requirements of a hybrid IoT-/agent-based business process orchestration architecture that utilizes open standards. We propose a four-layered architecture, which integrates autonomous agents and business process orchestration for IoT/agents, and provide a running example for a supply chain management (purchasing) use case.","Business Process Management, Internet of Things, Orchestration"
Agile Generator-Based GUI Modeling for Information Systems,"We use two code generators for the model-based continuous development of information systems including its graphical user interfaces (GUIs). As our goal is to develop full-size real-world systems for different domains, the continuous and iterative model-based engineering of their GUIs comes along with challenges regarding their extension and modification. These challenges concern models, the languages they are written in and hand-written code. In this work we present four complementary approaches to allow extensions for GUIs that we encounter with the generator-based framework MontiGem to tackle these challenges. We discuss the four approaches in detail and present extensions of the framework in the grammar of the language, via atomic components, via hand-written amendments of generated models and by generating connections between the GUI and data structure models. These techniques can be used to create a flexible DSL for engineering information systems, adaptable for different domains and rapidly changing requirements.","Code generation, Information system, Model-Based Software Engineering, Modeling graphical user interfaces, MontiGem"
An Extension of the QUAMOCO Quality Model to Specify and Evaluate Feature-Dependent Non-Functional Requirements,"Features in a software system usually must satisfy different quality expectations originating from the various stakeholders of a software system, ranging from direct users to the software manufacturer. As an example, the design quality of the source code will likely be more important if the associated feature is frequently used by customers or if it has strategic value for the software manufacturer. In order to effectively approach these qualitative subtleties, we need a means to specify non-functional requirements on the level of individual software features. Fine-grained specification of non-functional requirements on feature level respects their individual relevance for a feature and facilitates deriving suitable constraints for them. Particularly in DevOps-driven software projects with operational data the fulfillment of non-functional requirements for individual features can then continuously be evaluated. This guides software engineers in meeting these quality expectations on a much finer level than if treated uniformly across the software system. In this paper we present an extension of the QUAMOCO meta quality model for specifying and evaluating non-functional requirements on feature level. While the existing meta model focused on static quality measures, the extension of the meta model now also captures dynamic measures which accumulate during feature execution. An exploratory case study based on the results from an interview study shows the completeness of the approach for specifying and evaluating feature-dependent non-functional requirements.","Constraint Specification, Non-Functional Requirements, Operational Software Quality, Quality Modeling"
An Infrastructure Modelling Tool for Cloud Provisioning,"Cloud computing offers computing, network, and storage capabilities through services that abstract the capabilities of the underlying hardware. Currently, a variety of tools exist that manage the infrastructure provisioning and use scripts to define the final state of the hardware to be deployed in the cloud. However, there are major challenges that need to be addressed to automate the infrastructure management so that they are effectively used in initiatives such as DevOps. In particular, the management of Infrastructure as a Code (IaC) is one of the most important technical challenges to support activities such as the integration, deployment, and continuous delivery of applications. To address this problem, we present a support for the management of DevOps tools, through the definition of a Domain Specific Language (DSL) based on the concept of Infrastructure as a Code, and a tool that supports this language allowing to model the final state of a provisioning infrastructure in the cloud and generating the provisioning scripts for the Amazon Web Services (AWS) platform. The proposed tool reduces the work for development and operations personnel and facilitates their communication.","Cloud Services, DevOps, Infrastructure as Code, Infrastructure Provisioning, Model Driven Development"
An MDE Method for Improving Deep Learning Dataset Requirements Engineering using Alloy and UML,"Since the emergence of deep learning (DL) a decade ago, only few software engineering development methods have been defined for systems based on this machine learning approach. Moreover, rare are the DL approaches addressing specifically requirements engineering. In this paper, we define a model-driven engineering (MDE) method based on traditional requirements engineering to improve datasets requirements engineering. Our MDE method is composed of a process supported by tools to aid customers and analysts in eliciting, specifying and validating dataset structural requirements for DL-based systems. Our model driven engineering approach uses the UML semi-formal modeling language for the analysis of datasets structural requirements, and the Alloy formal language for the requirements model execution based on our informal translational semantics. The model executions results are then presented to the customer for improving the dataset validation activity. Our approach aims at validating DL-based dataset structural requirements by modeling and instantiating their datatypes. We illustrate our approach with a case study on the requirements engineering of the structure of a dataset for classification of five-segments digits images.","Alloy, EMF, Model-driven Engineering, Requirements Engineering, Sirius, Software Engineering"
AppFlow: Using machine learning to synthesize robust, reusable UI tests,"UI testing is known to be difficult, especially as today's development cycles become faster. Manual UI testing is tedious, costly and errorprone. Automated UI tests are costly to write and maintain. This paper presents AppFlow, a system for synthesizing highly robust, highly reusable UI tests. It leverages machine learning to automatically recognize common screens and widgets, relieving developers from writing ad hoc, fragile logic to use them in tests. It enables developers to write a library of modular tests for the main functionality of an app category (e.g., an ""add to cart"" test for shopping apps). It can then quickly test a new app in the same category by synthesizing full tests from the modular ones in the library. By focusing on the main functionality, AppFlow provides ""smoke testing"" requiring little manual work. Optionally, developers can customize AppFlow by adding app-specific tests for completeness. We evaluated AppFlow on 60 popular apps in the shopping and the news category, two case studies on the BBC news app and the JackThreads shopping app, and a user-study of 15 subjects on the Wish shopping app. Results show that AppFlow accurately recognizes screens and widgets, synthesizes highly robust and reusable tests, covers 46.6% of all automatable tests for Jackthreads with the tests it synthesizes, and reduces the effort to test a new app by up to 90%. Interestingly, it found eight bugs in the evaluated apps, including seven functionality bugs, despite that they were publicly released and supposedly went through thorough testing.","machine learning, mobile testing, test reuse, test synthesis, UI recognition, UI testing"
Architectural runtime models for integrating runtime observations and component-based models,"Keeping track of modern software applications while dynamically changing requires strong interaction of evolution activities on development level and adaptation activities on operation level. Knowledge about software architecture is key for both, developers while evolving the system and operators while adapting the system. Existing architectural models used in development differ from those used in operation in terms of purpose, abstraction and content. Consequences are limited reuse of development models during operation, lost architectural knowledge and limited phase-spanning consideration of software architecture. In this paper, we propose modeling concepts of the iObserve approach to align architectural models used in development and operation. We present a correspondence model to bridge the divergent levels of abstraction between implementation artifacts and component-based architectural models. A transformation pipeline uses the information stored in the correspondence model to update architectural models based on changes during operation. Moreover, we discuss the modeling of complex workload based on observations during operation. In a case study-based evaluation, we examine the accuracy of our models to reflect observations during operation and the scalability of the transformation pipeline. Evaluation results show the accuracy of iObserve. Furthermore, evaluation results indicate iObserve adequately scales for some cases but shows scalability limits for others.","Palladio Component Model, Performance model, Runtime model, Software architecture, Workload"
Architecture violations detection and visualization in the continuous integration pipeline,"New code in projects can introduce violations that deviate the code implementation from the intended architecture. This process is known as architecture erosion. In this article, we propose an approach for recovering the implemented architecture, and detecting violations when comparing it with the intended architecture. Given a code repository, the continuous integration pipeline calls the solution to detect the incidences of architecture violations as well as some quality and social metrics. This data is presented in metric-centered views that help development teams to manage architecture erosion. Our approach is based on model-driven engineering techniques since models serve to represent the code, and a model-based pattern language helps us to automate the search for violation occurrences and execute corresponding actions (e.g., creation/assignment of issues). We confirm the approach benefits in a real project implemented by a software developing company, in a sample project available on the internet, and in a software development course, including 20 projects, where every single project decreases its architecture violations density through time.","architecture discovering, architecture recovering, conformance checking, continuous integration"
Artifact and reference models for generative machine learning frameworks and build systems,"Machine learning is a discipline which has become ubiquitous in the last few years. While the research of machine learning algorithms is very active and continues to reveal astonishing possibilities on a regular basis, the wide usage of these algorithms is shifting the research focus to the integration, maintenance, and evolution of AI-driven systems. Although there is a variety of machine learning frameworks on the market, there is little support for process automation and DevOps in machine learning-driven projects. In this paper, we discuss how metamodels can support the development of deep learning frameworks and help deal with the steadily increasing variety of learning algorithms. In particular, we present a deep learning-oriented artifact model which serves as a foundation for build automation and data management in iterative, machine learning-driven development processes. Furthermore, we show how schema and reference models can be used to structure and maintain a versatile deep learning framework. Feasibility is demonstrated on several state-of-the-art examples from the domains of image and natural language processing as well as decision making and autonomous driving.","artifact models, artificial intelligence, build systems, compiler, machine learning, metamodeling, reference models, training"
Artshop: A continuous integration and quality assessment framework for model-based software artifacts,"Due to the increasing amount of features, software developer within the automotive domain are confronted with steadily increasing complexity of models within their respective model-based development tools. This also impacts the cost and time needed to perform manual maintenance and quality control during the evolution of model-based software, delaying the detection and correction of quality defects. To prevent quality defects to pervade a model, quality assessment should continuously be performed, to provide rapid feedback to the developers. This paper introduces a framework to enable integrated quality assessment during the evolution of model-based software artifacts. The framework includes a model repository supporting the import of data-flow diagrams from The Mathwork's Matlab-Simulink/Stateflow, formal modules from IBM Rational DOORS and variability documentations from pure:systems pure:variants, as well as the synchronization of imported models with their initial sources. Analyses provided by the framework range from structural and semantical checks of MATLAB/Simulink models to consistency and user-defined conformity checks across all supported artifact types. Results computed by the framework are stored alongside the revisions of the artifacts they refer to.","Artifact integration, Model-based development, Quality assessment"
Arttest - A New Test Environment for Model-Based Software Development,"Modern vehicles become increasingly software intensive. Software development therefore is critical to the success of the manufacturer to develop state of the art technology. Standards like ISO 26262 recommend requirement-based verification and test cases that are derived from requirements analysis. Agile development uses continuous integration tests which rely on test automation and evaluation. All these drove the development of a new model-based software verification environment. Various aspects had to be taken into account: the test case specification needs to be easily comprehensible and flexible in order to allow testing of different functional variants. The test environment should support different use cases like open-loop or closed-loop testing and has to provide corresponding evaluation methods for continuously changing as well as for discrete signals. In a joint project of RWTH Aachen University and Ford, a new tool, Arttest, has been developed for testing model-based software. The tool uses a domain specific language to specify the tests. It offers different test evaluation methods for automated open- and closed-loop testing and reactive testing. It automatically executes the tests, evaluates the outputs and generates summary reports indicating passed tests and errors found. The paper presents the tool and its various unique propositions such as domain specific test language, the evaluation properties and other features like open-loop and closed-loop capabilities.",
Auto-generation of domain-specific systems: Cloud-hosted devops for business,"The wide use of spreadsheet-based solutions for business processes illustrates the importance of giving business users simple mechanisms for specifying and managing their processes. However, spreadsheet-based solutions are hard to maintain, reuse, integrate, and scale. This paper describes an approach for supporting 'DevOps for business users' that enables business-level users to manage the full lifecycle of a large class of cloud-hosted business processes. The approach builds on DevOps for software engineering, but removes software engineers from the loop. Unlike general-purpose 'low code' business process management systems, the approach incorporates aspects of a processing domain (e.g., billing) to create a DevOps experience that business users can master easily. In the approach, business users follow an agile 'specify-check-generate-deploy' methodology, enabling them to rapidly and iteratively generate and operationalize cloud-hosted processing systems, with little or no assistance from IT staff. We demonstrate and evaluate the approach using a system built for the billing application area, developed at IBM, which provides technology support and maintenance services for numerous clients, each with different billing needs and logic. The paper describes the system, requirements, empirical evaluation of key components, and lessons learned.","business rules language, Cloud application, code generation, deployment automation, DevOps, model driven engineering"
Automated Classification of Metamodel Repositories: A Machine Learning Approach,"Manual classification methods of metamodel repositories require highly trained personnel and the results are usually influenced by the subjectivity of human perception. Therefore, automated metamodel classification is very desirable and stringent. In this work, Machine Learning techniques have been employed for metamodel automated classification. In particular, a tool implementing a feed-forward neural network is introduced to classify metamodels. An experimental evaluation over a dataset of 555 metamodels demonstrates that the technique permits to learn from manually classified data and effectively categorize incoming unlabeled data with a considerably high prediction rate: the best performance comprehends 95.40% as success rate, 0.945 as precision, 0.938 as recall, and 0.942 as F1 score.","Machine learning, metamodel classification, metamodel repositories"
Automated derivation of configurations for the integration of software(+) engineering environments,"Today's systems integration technologies enable the integration of (software+) engineering environments to support engineering processes across domain and tool boundaries. These engineering processes heavily rely on manual configuration of integration frameworks, resulting in costly, time-consuming, and error-prone human work. In this paper, we introduce an extended model-driven approach for the automated derivation of integration technology configurations for supporting engineering processes. This allows both an efficient and effective derivation of initial configurations, as well as easy adaptations of existing configurations in case of changed engineering processes. Based on a standard software engineering process, we show the feasibility of the proposed approach and discuss the advantages and limitations for software(+) engineering.","Automated configuration, Engineering domains, Model-driven approach, Systems integration"
Automated Regression Tests: A No-Code Approach for BPMN-based Process-Driven Applications,"BPMN-based Process-Driven Applications (PDA) require less coding since they are not only based on source code, but also on executable process models. Automated testing of such model-driven applications gains growing relevance, and it becomes a key enabler if we want to found their development on continuous integration (CI) techniques.While process analysts are typically responsible for test case specifications from a business perspective, technically skilled process engineers take the responsibility for implementing the required test code. This is time-consuming and, due to their often different skills and backgrounds, might result in communication problems such as information losses and misunderstandings. This paper presents a new approach which enables an analyst to generate executable tests for PDAs without the need for manual coding. It consists of a sophisticated model analysis, a wizard-based specification of test cases, and a subsequent code generation. The resulting tests can easily be integrated into CI pipelines.The concept is underpinned by a user-friendly tool which has been evaluated in case studies and in real-world implementation projects from different industry sectors. During the evaluation, the prototype proved a more efficient test creation process and a higher test quality.","BPMN, Model-Based Testing, No-Code, Process-Driven Application"
Automating performance bottleneck detection using search-based application profiling,"Application profiling is an important performance analysis technique, when an application under test is analyzed dynamically to determine its space and time complexities and the usage of its instructions. A big and important challenge is to profile nontrivial web applications with large numbers of combinations of their input parameter values. Identifying and understanding particular subsets of inputs leading to performance bottlenecks is mostly manual, intellectually intensive and laborious procedure. We propose a novel approach for automating performance bottleneck detection using search-based input-sensitive application profiling. Our key idea is to use a genetic algorithm as a search heuristic for obtaining combinations of input parameter values that maximizes a fitness function that represents the elapsed execution time of the application. We implemented our approach, coined as Genetic Algorithm-driven Profiler (GA-Prof) that combines a search-based heuristic with contrast data mining of execution traces to accurately determine performance bottlenecks. We evaluated GA-Prof to determine how effectively and efficiently it can detect injected performance bottlenecks into three popular open source web applications. Our results demonstrate that GA-Prof efficiently explores a large space of input value combinations while automatically and accurately detecting performance bottlenecks, thus suggesting that it is effective for automatic profiling.","Application profiling, Performance bottlenecks"
Automation of the incremental integration of microservices architectures,"Microservices have appeared as a new architectural style that is native to the cloud. The high availability and agility of the cloud demands organizations to migrate or design microservices, promoting the building of applications as a suite of small and cohesive services that are independently developed, deployed and scaled. Current cloud development approaches do not support the incremental integration needed for microservice platforms, and the agility of getting new functionalities out to customers is consequently affected by the lack of support for the integration design and automation of the development and deployment tasks. This paper presents an approach for the incremental integration of microservices that will allow architects to specify and design microservice integration, and provide mechanisms to automatically obtain the implementation code for business logic and interoperation among microservices, along with deployment and architectural reconfiguration scripts specific to the cloud environment in which the microservice will be deployed.","Cloud, Cloud architectures, Incremental, Integration, Microservices architectures"
Automotive Architecture Framework: The experience of Volvo Cars,"The automotive domain is living an extremely challenging historical moment shocked by many emerging business and technological needs. Electrification, autonomous driving, and connected cars are some of the driving needs in this changing world. Increasingly, vehicles are becoming software-intensive complex systems and most of the innovation within the automotive industry is based on electronics and software. Modern vehicles can have over 100 Electronic Control Units (ECUs), which are small computers, together executing gigabytes of software. ECUs are connected to each other through several networks within the car, and the car is increasingly connected with the outside world. These novelties ask for a change on how the software is engineered and produced and for a disruptive renovation of the electrical and software architecture of the car. In this paper we describe the current investigation of Volvo Cars to create an architecture framework able to cope with the complexity and needs of present and future vehicles. Specifically, we present scenarios that describe demands for the architectural framework and introduce three new viewpoints that need to be taken into account for future architectural decisions: Continuous Integration and Deployment, Ecosystem and Transparency, and car as a constituent of a System of Systems. Our results are based on a series of focus groups with experts in automotive engineering and architecture from different companies and universities.","Architecture framework, Automotive domain, Automotive ecosystem, Continuous integration and deployment, Software architecture, Systems of Systems"
Autonomic Management Framework for Cloud-Native Applications,"In order to meet the rapidly changing requirements of the Cloud-native dynamic execution environment, without human support and without the need to continually improve ones skills, autonomic features need to be added. Embracing automation at every layer of performance management enables us to reduce costs while improving outcomes. The main contribution of this paper is the definition of autonomic management requirements of Cloud-native applications. We propose that the automation is achieved via high-level policies. In turn autonomy features are accomplished via the rule engine support. First, the paper presents the engineering perspective of building a framework for Autonomic Management of Cloud-Native Applications, namely AMoCNA, in accordance with Model Driven Architecture (MDA) concepts. AMoCNA has many desirable features whose main goal is to reduce the complexity of managing Cloud-native applications. The presented models are, in fact, meta-models, being technology agnostic. Secondly, the paper demonstrates one possibility of implementing the aforementioned design procedures. The presented AMoCNA implementation is also evaluated to identify the potential overhead introduced by the framework.","Autonomic Computing (AC), Cloud-native, Observability, Policy-driven management, Resource management"
Autonomic pervasive applications driven by abstract specifications,"Pervasive application architectures present stringent requirements that make their development especially hard. In particular, they need to be flexible in order to cope with dynamism in different forms (e.g. service and data providers and consumers). The current trend to build applications out of remote services makes the availability of constituent application components inherently dynamic. Developers can no longer assume that applications are static after development or at run time. Unfortunately, developing applications that are able to cope with dynamism is very complex. Existing development approaches do not provide explicit support for managing dynamism. In this paper we describe Rondo, a tool suite for designing pervasive applications. More specifically, we present our propositions in pervasive application specification, which borrows concepts from service-oriented component assembly, model-driven engineering (MDE) and continuous deployment, resulting in a more flexible approach than traditional application definitions. Then the capabilities of our application model are demonstrated with an example application scenario designed using our approach. Copyright 2012 ACM.","Autonomic computing, Internet of things, Pervasive computing, Service-oriented computing"
Bridging the model-to-code abstraction gap with fuzzy logic in model-based regression test selection,"Regression test selection (RTS) approaches reduce the cost of regression testing of evolving software systems. Existing RTS approaches based on UML models use behavioral diagrams or a combination of structural and behavioral diagrams. However, in practice, behavioral diagrams are incomplete or not used. In previous work, we proposed a fuzzy logic based RTS approach called FLiRTS that uses UML sequence and activity diagrams. In this work, we introduce FLiRTS 2, which drops the need for behavioral diagrams and relies on system models that only use UML class diagrams, which are the most widely used UML diagrams in practice. FLiRTS 2 addresses the unavailability of behavioral diagrams by classifying test cases using fuzzy logic after analyzing the information commonly provided in class diagrams. We evaluated FLiRTS 2 on UML class diagrams extracted from 3331 revisions of 13 open-source software systems, and compared the results with those of code-based dynamic (Ekstazi) and static (STARTS) RTS approaches. The average test suite reduction using FLiRTS 2 was 82.06%. The average safety violations of FLiRTS 2 with respect to Ekstazi and STARTS were 18.88% and 16.53%, respectively. FLiRTS 2 selected on average about 82% of the test cases that were selected by Ekstazi and STARTS. The average precision violations of FLiRTS 2 with respect to Ekstazi and STARTS were 13.27% and 9.01%, respectively. The average mutation score of the full test suites was 18.90%; the standard deviation of the reduced test suites from the average deviation of the mutation score for each subject was 1.78% for FLiRTS 2, 1.11% for Ekstazi, and 1.43% for STARTS. Our experiment demonstrated that the performance of FLiRTS 2 is close to the state-of-art tools for code-based RTS but requires less information and performs the selection in less time.","Class diagram, Fuzzy logic, Regression test selection, UML"
Cloud MF: Applying MDE to tame the complexity of managing multi-cloud applications,"The market of cloud computing encompasses an ever-growing number of cloud providers offering a multitude of infrastructure-as-a-service (IaaS) and platform-as-a-service (PaaS) solutions. The heterogeneity of these solutions hinders the proper exploitation of cloud computing since it prevents interoperability and promotes vendor lock-in, which increases the complexity of executing and managing multi-cloud applications (i.e., Applications that can be deployed across multiple cloud infrastructures and platforms). Providers of multi-cloud applications seek to exploit the peculiarities of each cloud solution and to combine the delivery models of IaaS and PaaS in order to optimise performance, availability, and cost. In this paper, we show how the Cloud Modelling Framework leverages upon model-driven engineering to tame this complexity by providing: (i) a tool-supported domain-specific language for specifying the provisioning and deployment of multi-cloud applications, and (ii) a models@run-time environment for enacting the provisioning, deployment, and adaptation of these applications.","Cloud computing, Cloud ML, Model-driven engineering, Multi-cloud"
CloudMF: Model-driven management of multi-cloud applications,"While the number of cloud solutions is continuously increasing, the development and operation of largescale and distributed cloud applications are still challenging. A major challenge is the lack of interoperability between the existing cloud solutions, which increases the complexity of maintaining and evolving complex applications potentially deployed across multiple cloud infrastructures and platforms. In this article, we show how the Cloud Modelling Framework leverages model-driven engineering and supports the DevOps ideas to tame this complexity by providing: (i) a domain-specific language for specifying the provisioning and deployment of multi-cloud applications, and (ii) a models@run-time environment for their continuous provisioning, deployment, and adaptation.","Cloud computing, DevOps, Model-driven engineering, Models@run-time, Multi-cloud"
Cognifying Model-Driven Software Engineering,"The limited adoption of Model-Driven Software Engineering (MDSE) is due to a variety of social and technical factors, which can be summarized in one: its (real or perceived) benefits do not outweigh its costs. In this vision paper we argue that the cognification of MDSE has the potential to reverse this situation. Cognification is the application of knowledge (inferred from large volumes of information, artificial intelligence or collective intelligence) to boost the performance and impact of a process. We discuss the opportunities and challenges of cognifying MDSE tasks and we describe some potential scenarios where cognification can bring quantifiable and perceivable advantages. And conversely, we also discuss how MDSE techniques themselves can help in the improvement of AI, Machine learning, bot generation and other cognification techniques.","AI, Bot, Machine learning, Model, Model-driven"
Consistency management in industrial continuous model-based development settings: a reality check,"This article presents the state of practice of consistency management in thirteen industrial model-based development settings. Our analysis shows a tight coupling between adopting shorter development cycles and increasingly pressing consistency management challenges. We find that practitioners desire to adopt shorter development cycles, but immature modeling practices slow them down. We describe the different patterns that emerge from the various industrial settings. There is an opportunity for researchers to provide practitioners with a migration path toward practices that enable more automated consistency management, and ultimately, continuous model-based development.","Agile, Consistency management, Model-based development"
Consistency reflection for automatic update of testing environment,"In this paper, we present an approach for maintaining consistency among design models, system under test, and test components. Our approach provides an automatic updating technique to deal with changes that happen frequently in agile software development. Consistency reflection by automatic updating allows us to cut off manual tasks required in maintaining automated tests and speed up testing process which is a bottleneck in service development lifecycle. We introduce the use of metadata, data structure that describes structural behavior of the system, in our automatic updating technique. Metadata is used as a common medium for communicating changes to keep consistency among design, system under test, and test components. We present an example of testing process to show how we apply this approach in practice. © 2012 IEEE.","Agile software development, Automated software testing, Automatic update, Behavior-driven development, Consistency reflection, Design models, Metadata"
Constraint-based behavioral consistency of evolving software systems,"Any complex software system exhibits a tension between the technical perspective required for its realization and the user-level perspective. We term this the how-what gap, represented by the questions how is a system implemented vs. what is its functionality/usage. The normative, anticipated behavior of a software system as envisaged during its development and the de facto, observed behavior emerging after its continued operation tends to drift apart, resulting in behavioral inconsistency. We discuss how behavioral consistency in software systems can be captured in technical and formal terms, we sketch a possible tool chain that could support it, and we describe some of the research challenges that must be solved. Our main idea is to combine software analysis approaches represented by various forms of static analysis and formal verification with runtime verification, monitoring, and automata learning in order to optimally leverage the de facto observed behaviour of the deployed systems.",
Continuous and Proactive Software Architecture Evaluation: An IoT Case,"Design-Time evaluation is essential to build the initial software architecture to be deployed. However, experts' assumptions made at design-Time are unlikely to remain true indefinitely in systems that are characterized by scale, hyperconnectivity, dynamism, and uncertainty in operations (e.g. IoT). Therefore, experts' design-Time decisions can be challenged at run-Time. A continuous architecture evaluation that systematically assesses and intertwines design-Time and run-Time decisions is thus necessary. This paper proposes the first proactive approach to continuous architecture evaluation of the system leveraging the support of simulation. The approach evaluates software architectures by not only tracking their performance over time, but also forecasting their likely future performance through machine learning of simulated instances of the architecture. This enables architects to make cost-effective informed decisions on potential changes to the architecture. We perform an IoT case study to show how machine learning on simulated instances of architecture can fundamentally guide the continuous evaluation process and influence the outcome of architecture decisions. A series of experiments is conducted to demonstrate the applicability and effectiveness of the approach. We also provide the architect with recommendations on how to best benefit from the approach through choice of learners and input parameters, grounded on experimentation and evidence.","Continuous evaluation, IoT, software architecture evaluation, time series forecasting"
Continuous architecting of stream-based systems,"Big data architectures have been gaining momentum in recent years. For instance, Twitter uses stream processing frameworks like Storm to analyse billions of tweets per minute and learn the trending topics. However, architectures that process big data involve many different components interconnected via semantically different connectors making it a difficult task for software architects to refactor the initial designs. As an aid to designers and developers, we developed OSTIA (On-the-fly Static Topology Inference Analysis) that allows: (a) visualising big data architectures for the purpose of design-time refactoring while maintaining constraints that would only be evaluated at later stages such as deployment and run-time, (b) detecting the occurrence of common anti-patterns across big data architectures, (c) exploiting software verification techniques on the elicited architectural models. This paper illustrates OSTIA and evaluates its uses and benefits on three industrial-scale case studies.",
Continuous deployment of trustworthy smart IoT systems,"While the next generation of IoT systems need to perform distributed processing and coordinated behaviour across IoT, Edge and Cloud infrastructures, their development and operation are still challenging. A major challenge is the high heterogeneity of their infrastructure, which broadens the surface for security attacks and increases the complexity of maintaining and evolving such complex systems. In this paper, we present our approach for Generation and Deployment of Smart IoT Systems (GeneSIS) to tame this complexity. GeneSIS leverages model-driven engineering to support the DevSecOps of Smart IoT Systems (SIS). More precisely, GeneSIS includes: (i) a domain specific modelling language to specify the deployment of SIS over IoT, Edge and Cloud infrastructure with the necessary concepts for security and privacy; and (ii) a models@run. time engine to enact the orchestration, deployment, and adaptation of these SIS. The results from our smart building case study have shown that GeneSIS can support security by design from the development (via deployment) to the operation of IoT systems and back again in a DevSecOps loop. In other words, GeneSIS enables IoT systems to keep up security and adapt to evolving conditions and threats while maintaining their trustworthiness.","DecSecOps, Deployment, DSL, IoT, MDE, Models@run. time"
Continuous Integration in Multi-view Modeling: A Model Transformation Pipeline Architecture for Production Systems Engineering,"Background. Systems modeling in Production Systems Engineering (PSE) is complex: Multiple views from different disciplines have to be integrated, while semantic differences stemming from various descriptions must be bridged. Aim. This paper proposes the Multi-view Modeling Framework (MvMF) approach and architecture of a model transformation pipeline. The approach aims to ease setup and shorten configuration effort of multi-view modeling operations and support the reusability of modeling environments, like additional view integration. Method. We combine multi-view modeling with principles from distributed, agile workflows, i.e., Git and Continuous Integration. Results. The MvMF provides a light-weight modeling operation environment for AutomationML (AML) models. We show MvMF capabilities and demonstrate the feasibility of MvMF with a demonstrating use case including fundamental model operation features, such as compare and merge. Conclusion. Increasing requirements on the traceability of changes and validation of system designs require improved and extended model transformations and integration mechanisms. The proposed architecture and prototype design represents a first step towards an agile PSE modeling workflow.","Domain-specific Languages, Domain-specific Modeling, Model Transformation, Model-driven Engineering, Multi-disciplinary Engineering, Production Systems Engineering"
Continuous learning of HPC infrastructure models using big data analytics and in-memory processing tools,"Exascale computing represents the next leap in the HPC race. Reaching this level of performance is subject to several engineering challenges such as energy consumption, equipment-cooling, reliability and massive parallelism. Model-based optimization is an essential tool in the design process and control of energy efficient, reliable and thermally constrained systems. However, in the Exascale domain, model learning techniques tailored to the specific supercomputer require real measurements and must therefore handle and analyze a massive amount of data coming from the HPC monitoring infrastructure. This becomes rapidly a 'big data' scale problem. The common approach where measurements are first stored in large databases and then processed is no more affordable due to the increasingly storage costs and lack of real-time support. Nowadays instead, cloud-based machine learning techniques aim to build on-line models using real-time approaches such as 'stream processing' and 'in-memory' computing, that avoid storage costs and enable fastdata processing. Moreover, the fast delivery and adaptation of the models to the quick data variations, make the decision stage of the optimization loop more effective and reliable. In this paper we leverage scalable, lightweight and flexible IoT technologies, such as the MQTT protocol, to build a highly scalable HPC monitoring infrastructure able to handle the massive sensor data produced by next-gen HPC components. We then show how state-of-the art tools for big data computing and analysis, such as Apache Spark, can be used to manage the huge amount of data delivered by the monitoring layer and to build adaptive models in real-time using on-line machine learning techniques.",
Continuous Systems and Software Engineering for Industry 4.0: A disruptive view,"Context: Industry 4.0 has substantially changed the manufacturing processes, leading to smart factories with full digitalization, intelligence, and dynamic production. The need for rigorous and continuous development of highly networked software-intensive Industry 4.0 systems entails great challenges. Hence, Industry 4.0 requires new ways to develop, operate, and evolve these systems accordingly. Objective: We introduce the view of Continuous Systems and Software Engineering for Industry 4.0 (CSSE I4.0). Method: Based on our research and industrial projects, we propose this novel view and its core elements, including continuous twinning, which is also introduced first in this paper. We also discuss the existing industrial engagement and research that could leverage this view for practical application. Results: There are still several open issues, so we highlight the most urgent perspectives for future work. Conclusions: A disruptive view on how to engineer Industry 4.0 systems must be established to pave the way for the realization of the fourth industrial revolution.","Continuous Software Engineering, Continuous Systems and Software Engineering, Industry 4.0"
Convolutional neural networks for enhanced classification mechanisms of metamodels,"© 2020 Elsevier Inc.Conventional wisdom on Model-Driven Engineering suggests that metamodels are crucial elements for modeling environments consisting of graphical editors, transformations, code generators, and analysis tools. Software repositories are commonly used in practice for locating existing artifacts provided that a classification procedure is available. However, the manual classification of metamodel in repositories produces results that are influenced by the subjectivity of human perception besides being tedious and prone to errors. Therefore, automated techniques for classifying metamodels stored in repositories are highly desirable and stringent. In this work, we propose MEMOCNN as a novel approach to classification of metamodels. In particular, we consider metamodels as data points and classify them using supervised learning techniques. A convolutional neural network has been built to learn from labeled data, and use the trained weights to group unlabeled metamodels. A comprehensive experimental evaluation proves that the proposal effectively categorizes input data and outperforms a state-of-the-art baseline.",
Cost-effective learning-based strategies for test case prioritization in continuous integration of highly-configurable software,"Highly-Configurable Software (HCSs) testing is usually costly, as a significant number of variants need to be tested. This becomes more problematic when Continuous Integration (CI) practices are adopted. CI leads the software to be integrated and tested multiple times a day, subject to time constraints (budgets). To address CI challenges, a learning-based test case prioritization approach named COLEMAN has been successfully applied. COLEMAN deals with test case volatility, in which some test cases can be included/removed over the CI cycles. Nevertheless, such an approach does not consider HCS particularities such as, by analogy, the volatility of variants. Given such a context, this work introduces two strategies for applying COLEMAN in the CI of HCS: the Variant Test Set Strategy (VTS) that relies on the test set specific for each variant; and the Whole Test Set Strategy (WST) that prioritizes the test set composed by the union of the test cases of all variants. Both strategies are applied to two real-world HCSs, considering three test budgets. Independently of the time budget, the proposed strategies using COLEMAN have the best performance in comparison with solutions generated randomly and by another learning approach from the literature. Moreover, COLEMAN produces, in more than 92% of the cases, reasonable solutions that are near to the optimal solutions obtained by a deterministic approach. Both strategies spend less than one second to execute. WTS provides better results in the less restrictive budgets, and VTS the opposite. WTS seems to better mitigate the problem of beginning without knowledge, and is more suitable when a new variant to be tested is added.","Continuous integration, Highly-configurable software, Software product line, Test case prioritization"
Deriving Microservice Code from Underspecified Domain Models Using DevOps-Enabled Modeling Languages and Model Transformations,"Domain-driven Design (DDD) is a model-based software design methodology, which focuses on close collaboration of domain experts and software engineers. It promotes to capture relevant domain concepts in domain models in order to define a ubiquitous language, whose terms are well understood by stakeholders and consistently used throughout the software engineering process. DDD is considered particularly useful in Microservice Architecture (MSA) engineering, because it specifies patterns to structure modeled domain concepts in isolated contexts. These contexts determine concepts' applicability and validity, and make relationships between domain concepts beyond context boundaries explicit. Consequently, DDD contexts may be used to prescribe microservices' granularities and interaction relationships. However, domain models are usually underspecified, which facilitates communication with domain experts, but hampers the derivation of microservice code.In this paper, we present a model-driven methodology towards deriving microservice code from underspecified domain models. It relies on a set of viewpoint-based MSA modeling languages, which respect the concerns of DevOps teams, and a set of automatic model transformations, which decrease modeling effort and systematize source code derivation. We validate the applicability of our methodology by means of two case studies.","Modeling of computer architecture, Services Architectures, Services Engineering"
Detecting architectural issues during the continuous integration pipeline,"The use of a software reference architecture limits possible deviations and errors in the implementation of software projects, as the code must follow predefined rules that developers must respect to guarantee quality. However, when introducing new code to projects these rules can be violated. As a result, architectural erosion, bad smells, or even bugs that can be difficult to find are introduced to the projects. This paper proposes an approach for reviewing compliance to predefined rules that map architectural decisions to code. During the continuous integration process, the automatic analysis raises an issue for each rule violation. Developers can analyze and correct issues, and trace/visualize improvements, or lack thereof, through time. We present a validation experiment carried out in the context of a Software Development course, and we show how the approach helps developers to write better code.","Architectural rules, Continuous integration, Issue identification, Issue visualization, Rule violation"
DevOps and Safety? SafeOps! Towards Ensuring Safety in Feature-Driven Development with Frequent Releases,"The increasing importance of software and rising level of connectivity of safety-critical products such as vehicles enable continuously improving and adding the functionality. DevOps development principles support such kind of continuous deployment. However, safety-critical products shall fulfill safety standards. In addition, it is impossible to show that a new or updated functionality is safe without considering the entire system. We introduce the SafeOps approach that leverages the DevOps principles automation, feature-driven development, and monitoring during operations to fulfill the requirements of the ISO 26262 when iteratively extending and improving safety-critical products. We present concepts and existing approaches to increase the level of automation of safety engineering tasks like safety analysis and generation of safety artifacts and we show how the management of these artifacts can be supported. Furthermore, we outline future research questions and propose a first concept to obtain quick and systematic feedback of the quality of the safety concept from the deployed products, enabling to enter the DevOps cycle from a safety point of view.","Agile, CD, Dependability, FMEA, FTA, MBSA, MBSE"
DevOps for IoT Systems: Fast and Continuous Monitoring Feedback of System Availability,"Current Internet-of-Things (IoT) systems are highly distributed systems, which integrate cloud, fog, and edge computing approaches. Accelerating their maintenance and continuous improvement, while ensuring their availability, is complex. DevOps promotes fast and continuous feedback from operations to development to detect problems before customers are impacted, among other benefits. However, there is not any formal definition of how to do this. This article defines the 'fast and continuous monitoring feedback of system availability' activity (FCF availability) that supports automatic and continuous monitoring feedback from operations to the development of the IoT system availability. This activity has been formalized through the software and systems process engineering metamodel (SPEM). Its implementation is demonstrated in a real scenario that provides evidence that the formalization of the FCF availability activity helps teams in better diagnosing and fixing outage problems. The result is a distributed and configurable monitoring component developed through code [monitoring as code (MaC)]. This component is embedded in the IoT infrastructure. MaC enables DevOps team to configure their own metrics and indicators at runtime, i.e., monitoring on demand. The formalization of this activity, based on an MaC technique, enables the automation, versioning, and replication of monitoring elements.","Availability, continuous monitoring, DevOps, fast and continuous feedback activity, Internet-of-Things (IoT) systems"
DevOpsML: Towards modeling DevOps processes and platforms,"DevOps and Model Driven Engineering (MDE) provide differently skilled IT stakeholders with methodologies and tools for organizing and automating continuous software engineering activities-from development to operations, and using models as key engineering artifacts, respectively. Both DevOps and MDE aim at shortening the development life-cycle, dealing with complexity, and improve software process and product quality. The integration of DevOps and MDE principles and practices in low-code engineering platforms (LCEP) are gaining attention by the research community. However, at the same time, new requirements are upcoming for DevOps and MDE as LCEPs are often used by non-technical users, to deliver fully functional software. This is in particular challenging for current DevOps processes, which are mostly considered on the technological level, and thus, excluding most of the current LCEP users. The systematic use of models and modeling to lowering the learning curve of DevOps processes and platforms seems beneficial to make them also accessible for non-technical users. In this paper, we introduce DevOpsML, a conceptual framework for modeling and combining DevOps processes and platforms. Tools along with their interfaces and capabilities are the building blocks of DevOps platform configurations, which can be mapped to software engineering processes of arbitrary complexity. We show our initial endeavors on DevOpsML and present a research roadmap how to employ the resulting DevOpsML framework for different use cases.","DevOps, Model-driven engineering, Modeling languages"
DICE: Quality-driven development of data-intensive cloud applications,"Model-driven engineering (MDE) often features quality assurance (QA) techniques to help developers creating software that meets reliability, efficiency, and safety requirements. In this paper, we consider the question of how quality-aware MDE should support data-intensive software systems. This is a difficult challenge, since existing models and QA techniques largely ignore properties of data such as volumes, velocities, or data location. Furthermore, QA requires the ability to characterize the behavior of technologies such as Hadoop/MapReduce, NoSQL, and stream-based processing, which are poorly understood from a modeling standpoint. To foster a community response to these challenges, we present the research agenda of DICE, a quality-aware MDE methodology for data-intensive cloud applications. DICE aims at developing a quality engineering tool chain offering simulation, verification, and architectural optimization for Big Data applications. We overview some key challenges involved in developing these tools and the underpinning models.","Big data, Model-driven engineering, Quality assurance"
Digital Twin-Based Cyber Range for Industrial Internet of Things,"With the continuous integration of information technology and operation technology, the Industrial Internet of Things (IIoT) is gradually changing from closed to open. Operators can configure, monitor, or control the industrial production process remotely via Internet, which brings security threats to IIoTs. Since the IIoT focuses on the availability of industrial production, it is unfeasible to study security issues directly on the industrial field. Thus, constructing an IIoT cyber range to reproduce industrial scenarios for offensive and defensive confrontation research is necessary. However, the traditional IIoT cyber range relies on physical industrial field devices that are not reproducible and hard to recover from cyber-attacks. To solve these problems, in this article, we propose a framework for a digital twin-based cyber range and a digital twin construction method with multiple models. Cyber ranges with digital twins are more flexible and convenient. Based on the proposed method, an industrial scenario is reproduced using machine learning algorithms to predict temperature changes from different perspectives. The experimental result shows the ability of digital twins to construct an IIoT cyber range to reproduce production processes and replace field devices.",
DQSOps: Data Quality Scoring Operations Framework for Data-Driven Applications,"Data quality assessment has become a prominent component in the successful execution of complex data-driven artificial intelligence (AI) software systems. In practice, real-world applications generate huge volumes of data at speeds. These data streams require analysis and preprocessing before being permanently stored or used in a learning task. Therefore, significant attention has been paid to the systematic management and construction of high-quality datasets. Nevertheless, managing voluminous and high-velocity data streams is usually performed manually (i.e. offline), making it an impractical strategy in production environments. To address this challenge, DataOps has emerged to achieve life-cycle automation of data processes using DevOps principles. However, determining the data quality based on a fitness scale constitutes a complex task within the framework of DataOps. This paper presents a novel Data Quality Scoring Operations (DQSOps) framework that yields a quality score for production data in DataOps workflows. The framework incorporates two scoring approaches, an ML prediction-based approach that predicts the data quality score and a standard-based approach that periodically produces the ground-truth scores based on assessing several data quality dimensions. We deploy the DQSOps framework in a real-world industrial use case. The results show that DQSOps achieves significant computational speedup rates compared to the conventional approach of data quality scoring while maintaining high prediction performance.","Automated data scoring, data assessment, data quality dimensions, DataOps, mutation testing"
Dynamic data management for continuous retraining,"Managing dynamic datasets intended to serve as training data for a Machine Learning (ML) model often emerges as very challenging, especially when data is often altered iteratively and already existing ML models should pertain to the data. For example, this applies when new data versions arise from either a generated or aggregated extension of an existing dataset a model has already been trained on. In this work, it is investigated on how a model-based approach for these training data concerns can be provided as well as how the complete process, including the resulting training and retraining process of the ML model, can therein be integrated. Hence, model-based concepts and the implementation are devised to cope with the complexity of iterative data management as an enabler for the integration of continuous retraining routines. With Deep Learning techniques becoming technically feasible and massively being developed further over the last decade, MLOps, aiming to establish DevOps tailored to ML projects, gained crucial relevance. Unfortunately, data-management concepts for iteratively growing datasets with retraining capabilities embedded in a model-driven ML development methodology are unexplored to the best of our knowledge. To fill in this gap, this contribution provides such agile data management concepts and integrates them and continuous retraining into the model-driven ML Framework MontiAnna [18]. The new functionality is evaluated in the context of a research project where ML is exploited for the optimal design of lattice structures for crash applications.","artificial intelligence, data management, model-driven engineering, retraining"
Dynamic Runtime Integration of New Models in Digital Twins,"The development of cyber-physical systems is heavily relying on model-driven approaches. After deployment, these models can be utilised in a Digital Twin setting, acting as virtual replicas of the physical components and reflecting the behaviour of the running system in real-time. Complex systems often consist of numerous models interacting with each other and individual models may need to be updated after deployment. This means that new models need to be integrated and swapped during runtime without interrupting the running system. In this paper, we propose an approach for model-based Digital Twins to replace individual models without stopping or halting the operation of a cyber-physical system. Furthermore, our approach allows to replace not only individual models, but also update the overall structure of the interaction of models in the Digital Twin setting. The use of the proposed mechanism is illustrated through two case-studies with an agricultural robot prototype.","Co-simulation, Digital Twins, Functional Mock-up Interface, Model Swap, Model-driven engineering"
EMMM: A Unified Meta-Model for Tracking Machine Learning Experiments,"Traditional software engineering tools for managing assetsspecifically, version control systemsare inadequate to manage the variety of asset types used in machine-learning model development experiments. Two possible paths to improve the management of machine learning assets include 1) Adopting dedicated machine-learning experiment management tools, which are gaining popularity for supporting concerns such as versioning, traceability, auditability, collaboration, and reproducibility; 2) Developing new and improved version control tools with support for domain-specific operations tailored to machine learning assets. As a contribution to improving asset management on both paths, this work presents Experiment Management Meta-Model (EMMM), a meta-model that unifies the conceptual structures and relationships extracted from systematically selected machine-learning experiment management tools. We explain the metamodels concepts and relationships and evaluate it using real experiment data. The proposed meta-model is based on the Eclipse Modeling Framework (EMF) with its meta-modeling language, Ecore, to encode model structures. Our meta-model can be used as a concrete blueprint for practitioners and researchers to improve existing tools and develop new tools with native support for machine-learning-specific assets and operations.","Machine learning experiments, Management tools, MDE, Metamodeling"
Enact: Development, operation, and quality assurance of trustworthy smart iot systems,"To unleash the full potential of IoT and flourishing innovations in application domains such as eHealth or smart city, it is critical to facilitate the creation and operation of trustworthy Smart IoT Systems (SIS). Since SIS typically operate in a changing and often unpredictable environment, the ability of these systems to continuously evolve and adapt to their new environment is decisive to ensure and increase their trustworthiness, quality and user experience. The DevOps movement advocates a set of software engineering best practices and tools, to ensure Quality of Service whilst continuously evolving complex systems. However, there is no complete DevOps support for trustworthy SIS today. In this paper we present a research roadmap to enable DevOps in such systems and introduce the ENACT DevOps concepts and Framework.","DevOps, Internet of things, Trustworthiness"
Evaluation of Graphical Modeling of CI/CD Workflows with Rig,"This evaluation is about our workshop on graphical modeling of CI/CD pipeline and how our self-developed tool Rig can support creating such workflows. We describe the three tasks the workshop covered and present the data we have gathered throughout the workshop via surveys of the participants. After that, the data is interpreted and discussed concerning its validity. Last, we draw conclusions from the data interpretations with regards to our future work with Rig and future workshops we plan to hold.","Continuous Integration and Deployment, DevOps, Domain-Specific Tools, Graphical Modeling, Language-Driven Engineering, Purpose-Specific Language, Software Engineering, Visual Authoring"
FLAMA: A collaborative effort to build a new framework for the automated analysis of feature models,"Nowadays, feature models are the de facto standard when representing commonalities and variability, with modern examples spanning up to 7000 features. Manual analysis of such models is challenging and error-prone due to sheer size. To help in this task, automated analysis of feature models (AAFM) has emerged over the past three decades. However, the diversity of these tools and their supported languages presents a significant challenge that motivated the MOD-EVAR community to initiate a project for a new tool that supports the UVL language. Despite the rise of machine learning and data science, along with robust Python-based libraries, most AAFM tools have been implemented in Java, creating a collaboration gap. This paper introduces Flama, an innovative framework that automates the analysis of variability models. It focuses on UVL model analysis and aims for easy integration and extensibility to bridge this gap and foster better community and cross-community collaboration.","data visualization, effective communication, graphs and tables, software product line, variability, visualization design process"
Fundamental requirements of a machine learning operations platform for industrial metal additive manufacturing,"Metal-based Additive Manufacturing (AM) can realize fully dense metallic components and thus offers an opportunity to compete with conventional manufacturing based on the unique merits possible through layer-by-layer processing. Unsurprisingly, Machine Learning (ML) applications in AM technologies have been increasingly growing in the past several years. The trend is driven by the ability of data-driven techniques to support a range of AM concerns, including in-process monitoring and predictions. However, despite numerous ML applications being reported for different AM concerns, no framework exists to systematically manage these ML models for AM operations in the industry. Moreover, no guidance exists on fundamental requirements to realize such a cross-disciplinary platform. Working with experts in ML and AM, this work identifies the fundamental requirements to realize a Machine Learning Operations (MLOps) platform to support process-based ML models for industrial metal AM (MAM). Project-level activities are identified in terms of functional roles, processes, systems, operations, and interfaces. These components are discussed in detail and are linked with their respective requirements. In this regard, peer-reviewed references to identified requirements are made available. The requirements identified can help guide small and medium enterprises looking to implement ML solutions for AM in the industry. Challenges and opportunities for such a system are highlighted. The system can be expanded to include other lifecycle phases of metallic and non-metallic AM.","Computing infrastructure, Data analytics and machine learning, Fundamental and functional requirements, Industrial additive manufacturing, Machine learning operations platform"
Graphical and textual model-driven microservice development,"Model-driven development (MDD) is an approach to software engineering that aims to enable analysis, validation, and code generation of software on the basis of models expressed with dedicated modeling languages. MDD is particularly useful in the engineering of complex, possibly distributed software systems. It is therefore sensible to investigate the adoption of MDD to support and facilitate the engineering of distributed software systems based on microservice architecture (MSA). This chapter presents recent insights from studying and developing two approaches for employing MDD in MSA engineering. The first approach uses a graphical notation to model the topology and interactions of MSA-based software systems. The second approach emerged from the first one and exploits viewpoint-based modeling to better cope with MSAs inherent complexity. It also considers the distributed nature of MSA teams, as well as the technology heterogeneity introduced by MSA adoption. Both approaches are illustrated and discussed in the context of a case study. Moreover, we present a catalog of research questions for subsequent investigation of employing MDD to support and facilitate MSA engineering.","Metamodeling, Microservice architecture, Model-driven development, Modeling languages, Software architecture, Software design"
Hardware in the loop simulation for product driven control of a cyber-physical manufacturing system,"Cyber-physical system (CPS) is considered as a building block of industry 4.0. They are formulated as a network of interacting cyberspace and physical elements. Dealing with this new industrial context, distributed control systems (DCS) are increasingly involved because they permit meeting flexibility and adaptability requirements, which can give scope to CPS. The product driven control system (PDS) is considered as DCS in which the product plays a major role in decision-making. However, the PDS paradigm has not yet received sufficient attention within the CPS. Relying on multi-agents system as implementation framework, radio frequency identity as auto-identity technologies, and hardware in the loop simulation as a practical methodology, the paper proposes a validation and practical framework of PDS applied to the highly automated flexible robotized assembly system. An efficient CPS is developed for a discrete flexible manufacturing system.","Cyber-physical systems, Distributed control system, Hardware in the loop simulation, Multi-agents system, Product driven system, RFID technologies"
Highly-optimizing and multi-target compiler for embedded system models :C++ compiler toolchain for the component and connector language Embeddedmontiarc,"Component and Connector (C&C) models, with their corresponding code generators, are widely used by large automotive manufacturers to develop new software functions for embedded systems interacting with their environment; C&C example applications are engine control, remote parking pilots, and traffic sign assistance. This paper presents a complete toolchain to design and compile C&C models to highly-optimized code running on multiple targets including x86/x64, ARM and WebAssembly. One of our contributions are algebraic and threading optimizations to increase execution speed for computationally expensive tasks. A further contribution is an extensive case study with over 50 experiments. This case study compares the runtime speed of the generated code using different compilers and mathematical libraries. These experiments showed that programs produced by our compiler are at least two times faster than the ones compiled by MATLAB/Simulink for machine learning applications such as image clustering for object detection. Additionally, our compiler toolchain provides a complete model-based testing framework and plug-in points for middleware integration. We make all materials including models and toolchains electronically available for inspection and further research.","Code generation, Model-driven software engineering"
Identification of microservices from monolithic applications through topic modelling,"Microservices emerged as one of the most popular architectural patterns in the recent years given the increased need to scale, grow and flexibilize software projects accompanied by the growth in cloud computing and DevOps. Many software applications are being submitted to a process of migration from its monolithic architecture to a more modular, scalable and flexible architecture of microservices. This process is slow and, depending on the project's complexity, it may take months or even years to complete. This paper proposes a new approach on microservice identification by resorting to topic modelling in order to identify services according to domain terms. This approach in combination with clustering techniques produces a set of services based on the original software. The proposed methodology is implemented as an open-source tool for exploration of monolithic architectures and identification of microservices. A quantitative analysis using the state of the art metrics on independence of functionality and modularity of services was conducted on 200 open-source projects collected from GitHub. Cohesion at message and domain level metrics' showed medians of roughly 0.6. Interfaces per service exhibited a median of 1.5 with a compact interquartile range. Structural and conceptual modularity revealed medians of 0.2 and 0.4 respectively. Our first results are positive demonstrating beneficial identification of services due to overall metrics' results.",
Impediments to Introducing Continuous Integration for Model-Based Development in Industry,"Model-based development and continuous integration each separately are methods to improve the productivity of development of complex modern software systems. We investigate industrial adoption of these two phenomena in combination, i.e., applying continuous integration practices in model-based development projects. Through semi-structured interviews, eleven engineers at three companies with different modelling practices share their views on perceived and experienced impediments to this adoption. We find some cases in which this introduction is undesired and expected to not be beneficial. For other cases, we find and categorize several impediments and discuss how they are dealt with in industrial practice. Model synchronization and tool interoperability are found the most challenging to overcome and the ways in which they are circumvented in practice are detrimental for introducing continuous integration.","Continuous integration, Interview study, Model-based development"
Improving Component Based Software Integration Testing Using Data Mining Technique,"To increase rapid and high-quality product delivery, companies divide applications into different components and reuse these components to reduce complexity in product development. This increases agility and flexibility in continuous integration within limited resources. Consequently, interaction among reusable components results in mismatch of components specification and retrieval due to encapsulated functionalities. Therefore, most of the components-based software fail during integration testing. In this paper, we propose a framework for component based software specification and integration testing. The proposed framework considers multiple contexts for component specification and uses a data mining technique for improving the integration testing process. We evaluated the proposed framework using empirical study. The results show significant improvement for various parameters in the proposed framework as compared to traditional approaches of component based software development.","CBSE, components, generation, MBT, Sequence Diagram, test case, testing, UML"
Improving invariant mining via static analysis,"This paper proposes the use of static analysis to improve the generation of invariants from test data extracted from Simulink models. Previous work has shown the utility of such automatically generated invariants as a means for updating and completing system specifications; they also are useful as a means of understanding model behavior. This work shows how the scalability and accuracy of the data mining process can be dramatically improved by using information from data/control flow analysis to reduce the search space of the invariant mining and to eliminate false positives. Comparative evaluations of the process show that the improvements significantly reduce execution time and memory consumption, thereby supporting the analysis of more complex models, while also improving the accuracy of the generated invariants.","Automated test generation, Invariant mining, Model-based development, Verification and validation"
Improving la redoute's CI/CD pipeline and devops processes by applying machine learning techniques,"The complexity inherent to software development and maintenance - not only in technical terms, but also from a human perspective - entails challenges that can be addressed as learning problems. Machine learning techniques may be employed as tools to gain insight about strategies that can lead to the improvement of the quality of software processes and products. Defect proneness prediction, in particular, may be identified as an active research field. As stated by DevOps guidelines, the possibility of obtaining quick feedback allows teams to operate in an agile mode in which communication, decision taking and problem solving are expeditious, allowing companies to boost business value. This paper describes ongoing research for applying machine learning techniques to improve the quality of processes and products inside the DevOps pipeline of the La Redoute's IT department.","CI/CD Pipeline, DevOps, Machine Learning, Software Defect Prediction, Software Development Life Cycle, Software Quality"
Improving model repair through experience sharing,"In model-driven software engineering, models are used in all phases of the development process. These models may get broken due to various editions throughout their life-cycle. There are already approaches that provide an automatic repair of models, however, the same issues might not have the same solutions in all contexts due to different user preferences and business policies. Personalization would enhance the usability of automatic repairs in different contexts, and by reusing the experience from previous repairs we would avoid duplicated calculations when facing similar issues. By using reinforcement learning we have achieved the repair of broken models allowing both automation and personalization of results. In this paper, we propose transfer learning to reuse the experience learned from each model repair. We have validated our approach by repairing models using different sets of personalization preferences and studying how the repair time improved when reusing the experience from each repair.","Model repair, Reinforcement learning, Transfer learning"
Incremental integration of microservices in cloud applications,"Microservices have recently appeared as a new architectural style that is native to the cloud. The high availability and agility of the cloud demands organizations to migrate or design microservices, promoting the building of applications as a suite of small and cohesive services (microservices) that are independently developed, deployed and scaled. Current cloud development approaches do not support the incremental integration needed for microservice platforms, and the agilityofgetting new functionalities out to customers is consequently affected by the lack of support for the integration design and automation of the development and deployment tasks. This paper presents an approach for the incremental integration of microservices that will allow developers to specify and design microservice integration, and provide mechanisms with which to automatically obtain the implementation code for business logic and interoperation among microservices along with deployment and architectural reconfiguration scripts specific to the cloud environment in which the microservice will be deployed.","Cloud, Cloud architectures, Incremental, Integration, Microservices"
Industrial requirements for supporting AI-enhanced model-driven engineering,"There is an increasing interest in research on the combination of AI techniques and methods with MDE. However, there is a gap between AI and MDE practices, as well as between researchers and practitioners. This paper tackles this gap by reporting on industrial requirements in this field. In the AIDOaRt research project, practitioners and researchers collaborate on AI-Augmented automation supporting modeling, coding, testing, monitoring, and continuous development in cyber-physical systems. The project specifically lies at the intersection of industry and academia collaboration with several industrial use cases. Through a process of elicitation and refinement, 78 high-level requirements were defined, and generalized into 30 generic requirements by the AIDOaRt partners. The main contribution of this paper is the set of generic requirements from the project for enhancing the development of cyber-physical systems with artificial intelligence, DevOps, and model-driven engineering, identifying the hot spots of industry needs in the interactions of MDE and AI. Future work will refine, implement and evaluate solutions toward these requirements in industry contexts.","artificial intelligence, cyber-physical systems, model-driven engineering, requirements"
Infrastructure as runtime models: Towards Model-Driven resource management,"The importance of continuous delivery and the emergence of tools allowing to treat infrastructure configurations programmatically have revolutionized the way computing resources and software systems are managed. However, these tools keep lacking an explicit model representation of underlying resources making it difficult to introspect, verify or reconfigure the system in response to external events. In this paper, we outline a novel approach that treats system infrastructure as explicit runtime models. A key benefit of using such models@run.time representation is that it provides a uniform semantic foundation for resources monitoring and reconfiguration. Adopting models at runtime allows one to integrate different aspects of system management, such as resource monitoring and subsequent verification into an unified view which would otherwise have to be done manually and require to use different tools. It also simplifies the development of various self-adaptation strategies without requiring the engineers and researchers to cope with low-level system complexities.","Biological system modeling, Cloud computing, Computational modeling, Monitoring, Object oriented modeling, Runtime, Virtual machining"
Interactive Learning Engineering Concepts in AutomationML,"In the era of digitization, industrial engineering process generates a vast amount of data. To organize, store, and exchange such data, dedicated international standards are developed, including the XML-based data format AutomationML (AML). AML is standardized as IEC 62714 and is recommended for managing data flow in continuous engineering. Nevertheless, engineering data is inherently heterogeneous, and the harmonization of various data sources presents a bottleneck in the vision of an integrated engineering toolchain. In this paper, we present AMLLearner - a semi-automated system for learning engineering concepts from AML data. The results of learning are formal ontological definitions of engineering artifacts that naturally serve as knowledge for information integration. Based on the previous works on learning in AML, this paper emphasizes the involvement of end users, i.e. domain experts, for providing suggestions and feedback to the learner. To show the interactivity of AMLLearner, we discuss its characteristics regarding the recent study on interactive machine learning systems.",
Learning by sampling: learning behavioral family models from software product lines,"Family-based behavioral analysis operates on a single specification artifact, referred to as family model, annotated with feature constraints to express behavioral variability in terms of conditional states and transitions. Family-based behavioral modeling paves the way for efficient model-based analysis of software product lines. Family-based behavioral model learning incorporates feature model analysis and model learning principles to efficiently unify product models into a family model and integrate the behavior of various products into a behavioral family model. Albeit reasonably effective, the exhaustive analysis of product lines is often infeasible due to the potentially exponential number of valid configurations. In this paper, we first present a family-based behavioral model learning techniques, called FFSMDiff. Subsequently, we report on our experience on learning family models by employing product sampling. Using 105 products of six product lines expressed in terms of Mealy machines, we evaluate the precision of family models learned from products selected from different settings of the T-wise product sampling criterion. We show that product sampling can lead to models as precise as those learned by exhaustive analysis and hence, reduce the costs for family model learning.","Family model, Model learning, Software product lines, T-wise sampling"
Learning-to-rank vs ranking-to-learn: Strategies for regression testing in continuous integration,"In Continuous Integr tion (CI), regression testing is constr ined by the time between commits. This dem nds for c reful selection nd/or prioritiz tion of test c ses within test suites too l rge to be run entirely. To this im, some M chine Le rning (ML) techniques h ve been proposed, s n ltern tive to deterministic ppro ches. Two bro d str tegies for ML-b sed prioritiz tion re le rning-tor nk nd wh t we c ll r nking-to-le rn (i.e., reinforcement le rning). V rious ML lgorithms c n be pplied in e ch str tegy. In this p per we introduce ten of such lgorithms for doption in CI pr ctices, nd perform comprehensive study comp ring them g inst e ch other using subjects from the Ap che Commons project.We n lyze the influence of sever l fe tures of the code under test nd of the test process. The results llow to dr w criteri to support testers in selecting nd tuning the technique th t best fits their context.","Continuous integration, Machine learning, Regression testing, Test prioritization, Test selection"
LUV is not the answer: Continuous delivery of a model driven development platform,"The OutSystems Platform is a visual model-driven development and delivery platform that allows developers to create enterprise-grade cross platform web and mobile applications. The platform consists of several inter-dependent components, most notably Service Studio, the Platform Server, and LifeTime. Service Studio is an integrated development environment used to create applications that are then compiled by the Platform Server. LifeTime is used to stage applications between different environments (e.g., development, testing, production). Our meta-model is versioned using a version number that we call Last Upgrade Version (LUV). Service Studio, the Platform Server, and the models they create/process are associated with a particular LUV. As a general rule, a platform component is only able to process models with the same LUV as the component itself. This approach is not very flexible: a change to the meta-model requires releasing a new set of platform components that our customers then need to install. Although there's low resistance to installing new versions of Service Studio, the same is not true for the Platform Server. Thus, for all practical purposes LUV changes are tied to releases of major versions of the OutSystems Platform. In this paper we share the techniques that allowed us to transition to a Continuous Delivery process in which our meta-model can evolve freely with no impact on our installed base.","Continuous delivery, Meta-model evolution, Model driven development, Model evolution"
Machine learning based predictive modeling to effectively implement DevOps practices in software organizations,"Development and Operations (DevOps) is a relatively recent phenomenon that can be defined as a multidisciplinary effort to improve and accelerate the delivery of business values in terms of IT solutions. Many software organizations are heading towards DevOps to leverage its benefits in terms of improved development speed, stability, collaboration, and communication. DevOps practices are essential to effectively implement in software organizations, but little attention has been given in the literature to how these practices can be managed. Our study aims to propose and develop a framework for effectively managing DevOps practices. We have conducted an empirical study using the publicly available HELENA2 dataset to identify the best practices for effectively implementing DevOps. Furthermore, we have used the prediction algorithms such as Support Vector Machine (SVM), Artificial Neural Network (ANN) and Random Forest (RF) to develop a prediction model for DevOps implementation. The findings of this study show that Continuous deployment, Coding standards, Continuous integration, and Daily Standup ""are the most significant practicesduring the life cycle of projects for effectively managing the DevOps practices. The contribution of this study is not only limited to investigating the best DevOps practices but also provides a prediction of DevOps project success and prioritization of best practices. It can assist software organizations in getting the best possible practices to focus on based on the nature of their projects.","Artificial neural network, DevOps, Prediction model, Random forest, Support vector machine"
Machine Learning Regression Techniques for Test Case Prioritization in Continuous Integration Environment,"Test Case Prioritization (TCP) techniques are a key factor in reducing the regression testing costs even more when Continuous Integration (CI) practices are adopted. TCP approaches based on failure history have been adopted in this context because they are more suitable for CI environment constraints: test budget and test case volatility, that is, test cases may be added or removed over the CI cycles. Promising approaches are based on Reinforcement Learning (RL), which learns with past prioritization, guided by a reward function. In this work, we introduce a TCP approach for CI environments based on the sliding window method, which can be instantiated with different Machine Learning (ML) algorithms. Unlike other ML approaches, it does not require retraining the model to perform the prioritization and any code analysis. As an alternative for the RL approaches, we apply the Random Forest (RF) algorithm and a Long Short Term Memory (LSTM) deep learning network in our evaluation. We use three time budgets and eleven systems. The results show the applicability of the approach considering the prioritization time and the time between the CI cycles. Both algorithms take just a few seconds to execute. The RF algorithm obtained the best performance for more restrictive budgets compared to the RL approaches described in the literature. Considering all systems and budgets, RF reaches Normalized Average Percentage of Faults Detected (NAPFD) values that are the best or statistically equivalent to the best ones in around 72% of the cases, and the LSTM network in 55% of them. Moreover, we discuss some implications of our results for the usage of the algorithms evaluated.","Continuous Integration, Machine Learning, Recurrent Neural Networks, Regression Testing"
MDE for Machine Learning-Enabled Software Systems: A Case Study and Comparison of MontiAnna and ML-Quadrat,"In this paper, we propose to adopt the MDE paradigm for the development of Machine Learning (ML)-enabled software systems with a focus on the Internet of Things (IoT) domain. We illustrate how two state-of-The-Art open-source modeling tools, namely MontiAnna and ML-Quadrat can be used for this purpose as demonstrated through a case study. The case study illustrates using ML, in particular deep Artificial Neural Networks (ANNs), for automated image recognition of handwritten digits using the MNIST reference dataset, and integrating the machine learning components into an IoT-system. Subsequently, we conduct a functional comparison of the two frameworks, setting out an analysis base to include a broad range of design considerations, such as the problem domain, methods for the ML integration into larger systems, and supported ML methods, as well as topics of recent intense interest to the ML community, such as AutoML and MLOps. Accordingly, this paper is focused on elucidating the potential of the MDE approach in the ML domain. This supports the ML-engineer in developing the (ML/software) model rather than implementing the code, and additionally enforces reusability and modularity of the design through enabling the out-of-The-box integration of ML functionality as a component of the IoT or cyber-physical systems.","artificial intelligence, domain specific modeling, machine learning, model-driven engineering, tools"
Measuring performance quality scenarios in big data analytics applications: A DevOps and domain-specific model approach,"Big data analytics (BDA) applications use advanced analysis algorithms to extract valuable insights from large, fast, and heterogeneous data sources. These complex BDA applications require software design, development, and deployment strategies to deal with volume, velocity, and variety (3vs) while sustaining expected performance levels. BDA software complexity frequently leads to delayed deployments, longer development cycles and challenging performance monitoring. This paper proposes a DevOps and Domain Specific Model (DSM) approach to design, deploy, and monitor performance Quality Scenarios (QS) in BDA applications. This approach uses high-level abstractions to describe deployment strategies and QS enabling performance monitoring. Our experimentation compares the effort of development, deployment and QS monitoring of BDA applications with two use cases of near mid-air collisions (NMAC) detection. The use cases include different performance QS, processing models, and deployment strategies. Our results show shorter (re)deployment cycles and the fulfillment of latency and deadline QS for micro-batch and batch processing.","Big data analytics, DevOps, Domain specific model, Performance quality scenarios, Software architecture"
Model maturity-based model service composition in cloud environments,"With the development of cloud computing (CC), service-oriented architecture (SOA), and container technology, modeling and simulation (M&S) resources, such as simulation software and different sorts of models, can be shared and reused in a cloud environment. Modeling and Simulation as a Service (MSaaS), as a new paradigm, supports sharing simulation models or modeling tools and has enabled a wide range of model reuse. However, reusing or combining some immature models may result in inefficient M&S activities or even false simulation results. To make sure the appropriate reuse and composition of simulation models in cloud environments, which is also termed as model service composition for simulation (MSCS), this paper incorporates model maturity with service cooperation as a metric to evaluate the quality of model composition in cloud. Then, as a multi-objective optimization problem with multiple constraints, the MSCS problem and its process are described in detail. To solve the MSCS problem, a novel evolutionary algorithm named CA-AO-NSGAII is proposed. In the algorithm, adaptive crossover and mutation operators, as well as probabilistic initialization are developed. Furthermore, a half-local search algorithm in an elitist mechanism is designed for efficient decision-making. To validate the performance of CA-AO-NSGAII, experiments with respect to four different cases are conducted. Results show that the proposed method for addressing MSCS issue is effective and feasible.","Cloud computing, Evolutionary algorithm, Model maturity, Model service composition for simulation (MSCS), Modeling and simulation (M&S), MSaaS"
Model-based Continuous Deployment of SIS,,
Model-Based DevOps: Foundations and Challenges,"Time-to-market and continuous improvement are key success indicators to deliver for Industry 4.0 Cyber-Physical Systems (CPSs). There is thus a growing interest in adapting DevOps approaches coming from software systems to CPSs. However, CPSs are made not only of software but also of physical parts that need to be monitored at runtime. In this paper, we claim that Model-Driven Engineering can facilitate DevOps for CPSs by automatically connecting a CPS design model to its runtime monitoring, in the form of a digital twin.","DevOps, digital twins, evolution of engineering models, model-based software engineering, models at runtime, self-adaptation"
Model-based fleet deployment in the IoTedgecloud continuum,"With the increasing computing and networking capabilities, IoT devices and edge gateways have become part of a larger IoTedgecloud computing continuum, where processing and storage tasks are distributed across the whole network hierarchy, not concentrated only in the cloud. At the same time, this also introduced continuous delivery practices to the development of software components for network-connected gateways and sensing/actuating nodes. These devices are placed on end users premises and are characterized by continuously changing cyber-physical contexts, forcing software developers to maintain multiple application versions and frequently redeploy them on a distributed fleet of devices with respect to their current contexts. Doing this correctly and efficiently goes beyond manual capabilities and requires an intelligent and reliable automated solution. This paper describes a model-based approach to automatically assigning multiple software deployment plans to hundreds of edge gateways and connected IoT devices implemented in collaboration with a smart healthcare application provider. From a platform-specific model of an existing edge computing platform, we extract a platform-independent model that describes a list of target devices and a pool of available deployment plans. Next, we use constraint solving to automatically assign deployment plans to devices at once with respect to their specific contexts. The result is transformed back into the platform-specific model and includes a suitable deployment plan for each device, which is then consumed by our engine to deploy software components not only on edge gateways but also on their downstream IoT devices with constrained resources and connectivity. We validate the approach with a fleet deployment prototype integrated into a DevOps toolchain used by the partner application provider. Initial experiments demonstrate the viability of the approach and its usefulness in supporting DevOps for edge and IoT software development.","Constraint solving, Device fleet, DevOps, IoT, Model-based software engineering, Software deployment"
Model-based fleet deployment of edge computing applications,"Edge computing brings software in close proximity to end users and IoT devices. Given the increasing number of distributed Edge devices with various contexts, as well as the widely adopted continuous delivery practices, software developers need to maintain multiple application versions and frequently (re-)deploy them to a fleet of many devices with respect to their contexts. Doing this correctly and efficiently goes beyond manual capabilities and requires employing an intelligent and reliable automated approach. Accordingly this paper describes a joint research with a Smart Healthcare application provider on a model-based approach to automatically assigning multiple software deployments to hundreds of Edge gateways. From a Platform-Specific Model obtained from the existing Edge computing platform, we extract a Platform-Independent Model that describes a list of target devices and a pool of available deployments. Next, we use constraint solving to automatically assign deployments to devices at once, given their specific contexts. The resulting solution is transformed back to the PSM as to proceed with software deployment accordingly. We validate the approach with a Fleet Deployment prototype integrated into the DevOps toolchain currently used by the application provider. Initial experiments demonstrate the viability of the approach and its usefulness in supporting DevOps in Edge computing applications.","device fleet, DevOps, IoT, model-based software engineering, software deployment"
Model-based product line evolution: An incremental growing by extension,"Model-Based Engineering (MBE) and Product Line Engineering (PLE) have been combined, to handle new system development constraints like: increasing complexity, higher product quality, faster time-to-market and cost reduction. As observed by some authors, the derivation of a product from product line shared core assets has been insufficiently addressed and can remain tedious in practice. We cope with this issue focusing on having a flexible and reactive model-based derivation, and propose an incremental evolution by extension of the product line coupled with this derivation activity. Process and tools bridge the gap between Application and Domain Engineering introducing a semi-automatic feedback to benefits from the developments made in the Application Engineering. The approach is applied to a model-based product line dedicated to Class diagrams, and is tooled within the Eclipse environment. Copyright 2012 ACM.","Design tools, Evolution by extension, Methodology, Model based engineering, Product derivation, Product line engineering"
Model-Driven continuous deployment for quality devops,"DevOps entails a series of software engineering strategies and tools that promise to deliver quality and speed at the same time with little or no additional expense. In our work we strived to enable a DevOps way of working, combining Model-Driven Engineering tenets with the challenges of de- livering a model-driven continuous deployment tool that al- lows quick (re-)deployment of cloud applications for the purpose of continuous improvement. This paper illustrates the DICER tool and elaborates on how it can bring about the DevOps promise and enable the quality-awareness.","Continuous deployment, Model-driven engineering, Quality-aware devops"
Model-driven ml-ops for intelligent enterprise applications: vision, approaches and challenges,"This paper explores a novel vision for the disciplined, repeatable, and transparent model-driven development and Machine-Learning operations (ML-Ops) of intelligent enterprise applications. The proposed framework treats model abstractions of AI/ML models (named AI/ML Blueprints) as first-class citizens and promotes end-to-end transparency and portability from raw data detection- to model verification, and, policy-driven model management. This framework is grounded on the intelligent Application Architecture (iA2) and entails a first attempt to incorporate requirements stemming from (more) intelligent enterprise applications into a logically-structured architecture. The logical separation is grounded on the need to enact MLOps and logically separate basic data manipulation requirements (data-processing layer), from more advanced functionality needed to instrument applications with intelligence (data intelligence layer), and continuous deployment, testing and monitoring of intelligent application (knowledge-driven application layer). Finally, the paper sets out exploring a foundational metamodel underpinning blueprint-model-driven MLOps for iA2 applications, and presents its main findings and open research agenda.","AI software engineering, Methodological support to AI, ML Blueprints, ML-Ops, TOSCA"
Model-driven orchestration for cloud resources,"Several DevOps tools have emerged to orchestrate cloud resources. However, inherent heterogeneity and complex implementation within these tools make it hard for DevOps users to design required resource-related artifacts. Currently, the defacto standard for cloud resource modeling and orchestration is TOSCA. Nonetheless, TOSCA is usually bound to TOSCA-compliant orchestration tools. Moreover, the actual integration between TOSCA and DevOps tools is still performed using costly coding and in ad-hoc manner. To resolve this, we believe that mapping and translation mechanisms between TOSCA and DevOps tools should be provided. In this paper, we propose a new model-driven approach for cloud resource orchestration. Our approach (i) adopts TOSCA to design resource-related artifacts regardless of a specific DevOps tool; (ii) enables a new model-driven translation technique that serves to translate the designed artifacts using TOSCA into DevOps specific artifacts and (iii) provides Connectors that intend to establish the bridge between DevOps-specific artifacts and the DevOps tools. Our approach provides a powerful enhancement to DevOps productivity and reusability by assisting toward a seamless integration between TOSCA and DevOps tools.","Artifact, MDE, Orchestration, TOSCA, Transformation"
Modeling and automated execution of application deployment tests,"In recent years, many deployment systems have been developed that process deployment models to automatically provision applications. The main objective of these systems is to shorten delivery times and to ensure a proper execution of the deployment process. However, these systems mainly focus on the correct technical execution of the deployment, but do not check whether the deployed application is working properly. Especially in DevOps scenarios where applications are modified frequently, this can quickly lead to broken deployments, for example, if a wrong component version was specified in the deployment model that has not been adapted to a new database schema. Ironically, even hardly noticeable errors in deployment models quickly result in technically successful deployments, which do not work at all. In this paper, we tackle these issues. We present a modeling concept that enables developers to define deployment tests directly along with the deployment model. These tests are then automatically run by a runtime after deployment to verify that the application is working properly. To validate the technical feasibility of the approach, we applied the concept to TOSCA and extended an existing open source TOSCA runtime.","Declarative Application Deployement, Model-based Testing, Test Automation, Testing, TOSCA"
Modeling and Training of Neural Processing Systems,"The field of deep learning has become more and more pervasive in the last years as we have seen varieties of problems being solved using neural processing techniques. Image analysis and detection, control, speech recognition, translation are only a few prominent examples tackled successfully by neural networks. Thereby, the discipline imposes a completely new problem solving paradigm requiring a rethinking of classical software development methods. The high demand for deep learning technology has led to a large amount of competing frameworks mostly having a Python interface - a quasi standard in the community. Although, existing tools often provide great flexibility and high performance, they still lack to deliver a completely domain oriented problem view. Furthermore, using neural networks as reusable building blocks with clear interfaces in productive systems is still a challenge. In this work we propose a domain specific modeling methodology tackling design, training, and integration of deep neural networks. Thereby, we distinguish between three main modeling concerns: architecture, training, and data. We integrate our methodology in a component-based modeling toolchain allowing one to employ and reuse neural networks in large software architectures.","deep learning, model-driven software engineering, neural networks"
Modeling deep reinforcement learning based architectures for cyber-physical systems,"Reinforcement learning is a sub-field of machine learning where an agent aims to learn a behavior or a policy maximizing a reward function by trial and error. The approach is particularly interesting for the design of autonomous cyber-physical systems such as self-driving cars. In this work we present a generative, domain-specific modeling framework for the design, training and integration of reinforcement learning systems. It consists of a neural network modeling language which is used to design the models to be trained, e.g. actor and critic networks, and a training language used to describe the training procedure and set the corresponding hyperparameters. The underlying component model allows the modeler to embed the trained networks in larger component & connector architectures. We illustrate our framework by the example of a self-driving racing car.","Cyber-physical systems, Domain-specific languages, Machine learning, Reinforcement learning"
Modeling devOps deployment choices using process architecture design dimensions,"DevOps is a software development approach that enables enterprises to rapidly deliver software product features through process automation, greater inter-team collaboration and increased efficiency introduced through monitoring and measuring activities. No two enterprise-adopted DevOps approaches would be similar as each enterprise has unique characteristics and requirements. At present, there is no structured method in enterprise architecture modeling that would enable enterprises to devise a DevOps approach suitable for their requirements while considering possible process reconfigurations. Any DevOps implementation can have variations at different points across development and operational processes and enterprises need to be able to systematically map these variation points and understand the tradeoffs involved in selecting one alternative over another. In this paper, we use our previously proposed Business Process Architecture modeling technique to express and analyze DevOps alternatives and help enterprises select customized DevOps processes that match their contexts and requirements.","Adaptive enterprise, Business process modeling, Devops, Enterprise modeling, Goal modeling, Software processes"
Modeling, validation, and continuous integration of software behaviours for embedded systems,"We propose to test software models with software models. Model-Driven Software Development proposes that software is to be constructed by developing high-level models that directly execute or generate most of the code. On the other hand, Test-Driven development proposes to produce tests that validate the functionality of the code. This paper brings both together by using Logic-Labeled Finite-State Machines to deploy executable models of embedded systems and also to configure the corresponding tests. The advantage is a much more efficient validation of the models, with more robust and durable representations, that ensure effective and efficient quality assurance throughout the development process, saving the costly exercise of formal model-checking until the system is complete enough to meet all requirements.","Finite-State Machines, Model-Driven Development, Real-Time Systems, Software Models, Test-Driven Development, Validation and Model-Checking"
Models@runtime for continuous design and deployment,"Nowadays, software systems are leveraging upon an aggregation of dedicated infrastructures and platforms, which leads to the design of large scale, distributed, and dynamic systems. The need to evolve and update such systems after delivery is often inevitable, for example, due to changes in the requirements, maintenance, or needs for advancing the quality of services such as scalability and performances.",
MontiThings: Model-Driven Development and Deployment of Reliable IoT Applications,"Internet of Things (IoT) applications are exposed to harsh conditions due to factors such as device failure, network problems, or implausible sensor values. We investigate how the inherent encapsulation of component and connector (C&C) architectures can be used to develop and deploy reliable IoT applications. Existing C&C languages for the development of IoT applications mainly focus on the description of architectures and the distribution of components to IoT devices. Furthermore, related approaches often pollute the models with low-level implementation details, tying the models to a particular platform and making them harder to understand. In this paper, we introduce MontiThings, a C&C language offering automatic error handling capabilities and a clear separation between business logic and implementation details. The error-handling methods presented in this paper can make C&C-based IoT applications more reliable without cluttering the business logic with error-handling code that is time-consuming to develop and makes the models hard to understand, especially for non-experts.","Architecture modeling, Code generation, Deployment, Internet of Things, Model-driven engineering"
Neural Language Models and Few Shot Learning for Systematic Requirements Processing in MDSE,"Systems engineering, in particular in the automotive domain, needs to cope with the massively increasing numbers of requirements that arise during the development process. The language in which requirements are written is mostly informal and highly individual. This hinders automated processing of requirements as well as the linking of requirements to models. Introducing formal requirement notations in existing projects leads to the challenge of translating masses of requirements and the necessity of training for requirements engineers. In this paper, we derive domain-specific language constructs helping us to avoid ambiguities in requirements and increase the level of formality. The main contribution is the adoption and evaluation of few-shot learning with large pretrained language models for the automated translation of informal requirements to structured languages such as a requirement DSL.","few-shot learning, model-driven engineering, model-driven requirements engineering, natural language processing"
On the engineering of ai-powered systems,"More and more tasks become solvable using deep learning technology nowadays. Consequently, the amount of neural network code in software rises continuously. To make the new paradigm more accessible, frameworks, languages, and tools keep emerging. Although, the maturity of these tools is steadily increasing, we still lack appropriate domain specific languages and a high degree of automation when it comes to deep learning for productive systems. In this paper we present a multi-paradigm language family allowing the AI engineer to model and train deep neural networks as well as to integrate them into software architectures containing classical code. Using input and output layers as strictly typed interfaces enables a seamless embedding of neural networks into component-based models. The lifecycle of deep learning components can then be governed by a compiler accordingly, e.g. detecting when (re-)training is necessary or when network weights can be shared between different network instances. We provide a compelling case study, where we train an autonomous vehicle for the TORCS simulator. Furthermore, we discuss how the methodology automates the AI development process if neural networks are changed or added to the system.","Deep learning, Mde, Neural networks"
openCAESAR: Balancing Agility and Rigor in Model-Based Systems Engineering,"Model-Based System Engineering (MBSE) employs models and formal languages to support development of complex (systems-of-) systems. NASA Jet Propulsion Laboratory (JPL) sees MBSE as a key approach to managing the complexity of system development. However, balancing agility and rigor in MBSE has been reported as a challenging task not yet addressed by modeling tools and frameworks. This is because existing MBSE approaches may enable agility but compromise rigor, or enhance rigor but impede agility. We discuss the challenges of balancing agility and rigor in MBSE across seven systems engineering architectural functions defined by the JPL Integrated Model-Centric Engineering (IMCE) initiative. We demonstrate how openCAESAR, an open-source MBSE methodology and framework created at JPL, can strike a balance between agility and rigor through a case study of the Kepler16b project and discussion of lessons learned from past projects.","Model-Based Systems Engineering, OML, Ontology-based Modeling, openCAESAR, Systems Engineering"
Pattern-based Modelling, Integration, and Deployment of Microservice Architectures,"Microservice-based architectures (MSAs) gained momentum in industrial and research communities since finer-grained and more independent components foster reuse and reduce time to market. However, to come from the design of MSAs to running applications, substantial knowledge and technology-specific expertise in the deployment and integration of microservices is needed. In this paper, we propose a model-driven and pattern-based approach for composing microservices, which facilitates the transition from architectural models to running deployments. Using a unified modelling for MSAs, including both their integration based on Enterprise Integration Patterns (EIPs) and deployment aspects, our approach enables automatically generating the artefacts for deploying microservice compositions. This helps abstracting away the underlying infrastructure including container orchestration platforms and middleware layer for service integration. To validate the feasibility of our approach, we illustrate its prototypical implementation, with Kubernetes used as container orchestration system and OpenFaaS used for managing integration logic, and we present a case study.","Enterprise Integration Pattern, Microservice Architecture, Model-driven Engineering, Service Composition"
Personalized and automatic model repairing using reinforcement learning,"When performing modeling activities, the chances of breaking a model increase together with the size of development teams and number of changes in software specifications. Model repair research mostly proposes two different solutions to this issue: fully automatic, non-interactive model repairing tools or support systems where the repairing choice is left to the developer's criteria. In this paper, we propose the use of reinforcement learning algorithms to achieve the repair of broken models allowing both automation and personalization. We validate our proposal by repairing a large set of broken models randomly generated with a mutation tool.","Model repair, Personalization, Reinforcement learning"
Pinset: A DSL for extracting datasets from models for data mining-based quality analysis,"Data mining techniques have been successfully applied to software quality analysis and assurance, including quality of modeling artefacts. Before such techniques can be used, though, data under analysis commonly need to be formatted into two-dimensional tables. This constraint is imposed by data mining algorithms, which typically require a collection of records as input for their computations. The process of extracting data from the corresponding sources and formatting them properly can become error-prone and cumbersome. In the case of models, this process is mostly carried out through scripts written in a model management language, such as EOL or ATL. To improve this situation, we present Pinset, a domain-specific language devised for the extraction of tabular datasets from software models. Pinset offers a tailored syntax and built-in facilities for common activities in dataset extraction. For evaluation, Pinset has been used on UML class diagrams to calculate metrics that can be employed as input for several fault-prediction algorithms. The use of Pinset for this calculations led to more compact and high-level specifications when compared to equivalent scripts written in generic model management languages.","Data Mining, Domain-Specific Languages, Model-Driven Engineering, Software Quality"
Process implications of executable domain models for microservices development,"Microservice architecture has been recognized as an important enabler for continuous development of many cloud-based systems. Code generation has been tried in the tool chain of building microservices. However, most existing tools generally do not consider the risks from continuous development. We have been developing a toolkit which generates microservices from application domain models. Our approach aligns development process to this toolkit and coordinates domain modeling activity over project life cycles. In this paper, we describe its framework and corresponding development process which eliminates delays brought by the uncertainty of a project at a relatively early stage. Several minimum viable products have been built upon the proposed approach during the past years, including automated generation of code from domain decomposition. Our result shows 10% saving of effort and fewer issues. Effort saving increases to 30% under an extreme condition with high-rate personnel turnover. We also discuss our findings on running these projects and raise discussion and questions for future enhancement.","agile, code generation, continuous development, domain modeling, microservices"
Realization of a model-based DevOps process for industrial safety critical cyber physical systems,"Safety critical Industrial Cyber Physical Systems (CPS) have stringent safety and security requirements and need assurance of deterministic behavior during system operation. In many safety critical application domains, runtime monitors (or runtime verification) are used to enforce operational safety and security. One of the challenges in runtime verification is to identify the critical safety properties that we want to monitor at runtime. In this paper, we explore how structural verification activities in a Model Based Design and Engineering (MBDE) context help formulate more effective monitoring specifications to cover vulnerable areas in a system. We assert that leveraging synergy between design and runtime verification produces more informed runtime safety monitors. This approach of integrating design assurance and runtime safety and security is an important aspect of the dependable DevOps continuum process. To demonstrate this, we perform verification of an Emergency Diesel Generator Startup Sequencer (EDGSS) implemented on an FPGA overlay architecture using model-based verification techniques. We present our key findings on synergy between runtime verification and design processes to support a more inclusive safety case.","DevOps, Industrial Cyber Physical Systems, Model-based engineering, Runtime verification"
Refactoring architecture models for compliance with custom requirements,"In the process of software-intensive systems engineering, architectures need to be designed that are compliant to the requirements. For this, architects need to examine those requirements with regard to their architectural impact. Accessing and interpreting the requirements is however not always possible, for instance if custom requirements are yet unknown at the time when the architecture is modeled. Ideally, architectural knowledge as derived from custom requirements could be imposed upon architecture models. This paper proposes a novel concept for automated refactoring of architecture models in order to meet such requirements by formalizing architectural knowledge using model verification and model transformations. Industrial application within a telecommunications service provider is demonstrated in the domain of cloud application orchestration: service providers are enabled to autonomously customize solutions predefined by vendors according to their own internal requirements.",
Reinforcement learning for automatic test case prioritization and selection in continuous integration,"Testing in Continuous Integration (CI) involves test case prioritization, selection, and execution at each cycle. Selecting the most promising test cases to detect bugs is hard if there are uncertainties on the impact of committed code changes or, if traceability links between code and tests are not available. This paper introduces Retecs, a new method for automatically learning test case selection and prioritization in CI with the goal to minimize the round-trip time between code commits and developer feedback on failed test cases. The Retecs method uses reinforcement learning to select and prioritize test cases according to their duration, previous last execution and failure history. In a constantly changing environment, where new test cases are created and obsolete test cases are deleted, the Retecs method learns to prioritize error-prone test cases higher under guidance of a reward function and by observing previous CI cycles. By applying Retecs on data extracted from three industrial case studies, we show for the first time that reinforcement learning enables fruitful automatic adaptive test case selection and prioritization in CI and regression testing.","Continuous Integration, Machine Learning, Regression testing, Reinforcement Learning, Test case prioritization, Test case selection"
ResyDuo: Combining Data Models and CF-Based Recommender Systems to Develop Arduino Projects,"While specifying an IoT-based system, software developers have to face a set of challenges, spanning from selecting the hardware components to writing the actual source code. Even though dedicated development environments are in place, a non-expert user might struggle with the over-choice problem in selecting the proper component. By combining MDE and recommender systems, this paper proposes an initial prototype, called ResyDuo, to assist Arduino developers by providing two different artifacts, i.e., hardware components and software libraries. In particular, we make use of a widely adopted collaborative filtering algorithm by collecting relevant information by means of a dedicated data model. ResyDuo can retrieve hardware components by using tags or existing Arduino projects stored on the ProjectHub repository. Then, the system can eventually retrieve corresponding software libraries based on the identified hardware devices. ResyDuo is equipped with a web-based interface that allows users to easily select and configure the under-developing Arduino project. To assess ResyDuo's performances, we run the ten-fold cross-validation by adopting the grid search strategy to optimize the hyperparameters of the CF-based algorithm. The conducted evaluation shows encouraging results even though there is still room for improvement in terms of the examined metrics.","IoT development, Model-Driven Engineering, Recommendation Systems"
RLOps: Development Life-Cycle of Reinforcement Learning Aided Open RAN,"Radio access network (RAN) technologies continue to evolve, with Open RAN gaining the most recent momentum. In the O-RAN specifications, the RAN intelligent controllers (RICs) are software-defined orchestration and automation functions for the intelligent management of RAN. This article introduces principles for machine learning (ML), in particular, reinforcement learning (RL) applications in the O-RAN stack. Furthermore, we review the state-of-the-art research in wireless networks and cast it onto the RAN framework and the hierarchy of the O-RAN architecture. We provide a taxonomy for the challenges faced by ML/RL models throughout the development life-cycle: from the system specification to production deployment (data acquisition, model design, testing and management, etc.). To address the challenges, we integrate a set of existing MLOps principles with unique characteristics when RL agents are considered. This paper discusses a systematic model development, testing and validation life-cycle, termed: RLOps. We discuss fundamental parts of RLOps, which include: model specification, development, production environment serving, operations monitoring and safety/security. Based on these principles, we propose the best practices for RLOps to achieve an automated and reproducible model development process. At last, a holistic data analytics platform rooted in the O-RAN deployment is designed and implemented, aiming to embrace and fulfil the aforementioned principles and best practices of RLOps.","data engineering, digital twins, machine learning, MLOps, O-RAN, reinforcement learning, RLOps"
Scaling agile mechatronics: An industrial case study,"The automotive industry is currently in a state of rapid change. The traditional mechanical industry has, forced by electronic revolution and global threats of climate change, transformed into a computerized electromechanical industry. A hybrid or electric car of 2013 can have, in the order of 100 electronic control units, running gigabytes of code, working together in a complex network within the car as well as being connected to networks in the world outside. This exponential increase of software has posed new challenges for the R&D organizations. In many cases the commonly used method of requirement engineering towards external suppliers in a waterfall process has shown to be unmanageable. Part of the solution has been to introduce more in-house software development and the new standardized platform for embedded software, AUTOSAR. During the past few years, Volvo Cars has focused on techniques and processes for continuous integration of embedded software for active safety, body functions, and motor and hybrid technology. The feedback times for ECU system test have decreased from months to, in the best cases, hours. Domain-specific languages (DSL), for both software and physical models, have been used to great extent when developing in-house embedded software at Volvo Cars. The main reasons are the close connection with mechatronic systems (motors, powertrain, servos, etc.), the advantage of having domain experts (not necessarily software experts) developing control software, and the facilitated reuse of algorithms. Model-driven engineering also provides a method for agile development and early learning in projects where hardware and mechanics usually are available only late. Model-based testing of the software is performed, both as pure simulation (MIL) and in hardware-in-the-loop (HIL) rigs, before it is deployed in real cars. This testing is currently being automated for several rigs, as part of the continuous integration strategy. The progress is, however, not without challenges. Details of the work split with Tier 1 suppliers, using the young AUTOSAR standard, and the efficiency of AUTOSAR code are still open problems. Another challenge is to manage the complex model framework required for virtual verification when applied on system level and numerous DSLs have to be executed together.",
Scenarios in the loop: Integrated requirements analysis and automotive system validation,"The development of safety-relevant systems in the automotive industry requires the definition of high-quality requirements and tests for the coordination and monitoring of development activities in an agile development environment. In this paper we describe a Scenarios in the Loop (SCIL) approach. SCIL combines (1) natural language requirements specification based on Behavior-Driven Development (BDD) with (2) formal and test-driven requirements modeling and analysis, and (3) integrates discipline-specific tools for software and system validation during development. A central element of SCIL is a flexible and executable scenario-based modeling language, the Scenario Modeling Language for Kotlin (SMLK). SMLK allows for an intuitive requirements formalization, and supports engineers to move iteratively, and continuously aided by automated checks, from stakeholder requirements to the validation of the implemented system. We evaluated the approach using a real example from the field of e-mobility.","Automotive systems engineering, BizDevOps, Requirements analysis, System validation"
Self-healing multi-cloud application modelling,"Cloud computing market forecasts and technology trends confirm that Cloud is an IT disrupting phenomena and that the number of companies with multi-cloud strategy is continuously growing. Cost optimization and increased competitiveness of companies that exploit multi-cloud will only be possible when they are able to leverage multiple cloud offerings, while mastering both the complexity of multiple cloud provider management and the protection against the higher exposure to attacks that multi-cloud brings. .is paper presents the MUSA Security modelling language for multi-cloud applications which is based on the Cloud Application Modelling and Execution Language (CAMEL) to overcome the lack of expressiveness of state-of-the-art modelling languages towards easing: A) the automation of distributed deployment, b) the computation of composite Service Level Agreements (SLAs) that include security and privacy aspects, and c) the risk analysis and service match-making taking into account not only functionality and business aspects of the cloud services, but also security aspects. .e paper includes the description of the MUSA Modeller as the Web tool supporting the modelling with the MUSA modelling language. the paper introduces also the MUSA SecDevOps framework in which the MUSA Modeller is integrated and with which the MUSA Modeller will be validated.","Cloud, Deployment, Modelling, Multi-cloud, Security"
Simulation deployment blockset for MATLAB/Simulink,"Model-based approaches are being employed more and more in simulation development. Graphical modeling languages and code generation technologies are enabling agile model development workflows, so that simulation modelers can update their models more easily. However, the process from changing the model to releasing a new simulation version is overlooked. Simulation deployment can be defined as a collection of activities, including model checking, Model-in-the-Loop testing, code generation, build, Software-in-the-Loop testing, deployment, when applicable Processor-in-the-Loop and Hardware-in-the-Loop testing and release. When it is conducted manually and ad hoc, it is repetitive, labor intensive, time-consuming and error prone. The automation of deployment pipeline, on the other hand, requires extensive scripting, unfortunately, in way in which simulation modelers are usually not accustomed. Causal Block Diagrams propose a graphical modeling language that is extensively used in simulation of technical systems. MATLAB/Simulink supports them as the basic modeling language. Exploiting the competence of MATLAB/Simulink users on Causal Block Diagrams, this paper presents a model-based approach for automating the simulation deployment activities. Thus, rather than scripting, the deployment automation functions are made available and accessible to the simulation modelers within the graphical modeling environment that they are using.","Continuous delivery, Model-based simulation systems engineering, Simulation deployment"
Skyfire: Model-Based Testing with Cucumber,"In the software industry, a Behavior-Driven Development (BDD) tool, Cucumber, has been widely used by practitioners. Usually product analysts, developers, and testers manually write BDD test scenarios that describe system behaviors. Testers write implementation for the BDD scenarios by hand and execute the Cucumber tests. Cucumber provides transparency about what test scenarios are covered and how the test scenarios are mapped to executable tests. One drawback of the Cucumber BDD approach is that test scenarios are generated manually. Thus, the test scenarios are usually weak. More importantly, practitioners do not have a metric to measure test coverage. In this paper, we present a Model-Based Testing (MBT) tool, skyfire. Skyfire can automatically generate effective Cucumber test scenarios to replace manually generated test scenarios. Skyfire reads a behavioral UML diagram (e.g., a state machine diagram), identifies all necessary elements (e.g., transitions) of the diagram, generates effective tests to satisfy various graph coverage criteria, and converts the tests into Cucumber scenarios. Then testers write Cucumber mappings for the generated scenarios. Skyfire does not only generate effective tests but is also completely compatible with the existing agile development and continuous integration (CI) rhythm. We present the design architecture and implementation of skyfire, as well as an industrial case study to show how skyfire is used in practice.",
SliceOps: Explainable MLOps for Streamlined Automation-Native 6G Networks,"Sixth-generation (6G) network slicing is the backbone of future communications systems. It inaugurates the era of extreme ultra-reliable and low-latency communication (xURLLC) and pervades the digitalization of the various vertical immersive use cases. Since 6G inherently underpins artificial intelligence (AI), we propose a systematic and standalone slice termed SliceOps that is natively embedded in the 6G architecture, which gathers and manages the whole AI lifecycle through monitoring, re-training, and deploying the machine learning (ML) models as a service for the 6G slices. By leveraging machine learning operations (MLOps) in conjunction with eXplainable AI (XAI), SliceOps strives to cope with the opaqueness of black-box AI using explanation-guided reinforcement learning (XRL) to fulfill transparency, trustworthiness, and interpretability in the network slicing ecosystem. This article starts by elaborating on the architectural and algorithmic aspects of SliceOps. Then, the deployed cloud-native SliceOps working is exemplified via a latency-aware resource allocation problem. The deep RL (DRL)-based SliceOps agents within slices provide AI services aiming to allocate optimal radio resources and impede service quality degradation. Simulation results demonstrate the effectiveness of SliceOps-driven slicing. The article discusses afterward the SliceOps challenges and limitations. Finally, the key open research directions corresponding to the proposed approach are identified.","6G, AI, MLOps, network slicing, resource allocation, XAI, XRL, zero-touch"
SMADA-Fog: Semantic model driven approach to deployment and adaptivity in fog computing,"The deployment, monitoring and configuration of applications in Fog Computing are becoming quite challenging, due to heterogeneity of mobile and IoT devices involved, data movement constraints imposed by legal regulations as well as frequent changes in the execution environment that may affect quality of service. As a consequence, the system administration procedures are becoming more complex and time-consuming, especially if done manually. In this paper, a Semantic Model driven Approach to Deployment and Adaptivity of container-based applications in Fog Computing (SMADA-Fog) is proposed. Modeling tools, semantic framework, linear optimization model, simulation environment and infrastructure management code generator leveraging the semantic annotations are implemented and presented. According to results of the two experimentally tested scenarios, the proposed approach improves the application performance, while the time required for deployment as well as service adaptation is reduced for at least an order of magnitude.","DevOps, Fog Computing, Infrastructure as code, Linear optimization, Model-driven engineering, Semantic technology"
Smart Infrastructures: Artificial Intelligence-Enabled Lifecycle Automation,"The deployment and maintenance of large smart infrastructures used for powering data-driven decision making, regardless of retrofitted or newly deployed infrastructures, still lack automation and mostly rely on extensive manual effort. In this article, we focus on the two main challenges in the lifecycle of smart infrastructures: deployment and operation, each of which is rather generic and applies to all infrastructures. We discuss the existing technologies designed to help improve and automate deployment and operation for smart infrastructures in general and use the smart grid as a guiding example to ground some examples across the article. Next, we identify and discuss opportunities where the broad field of artificial intelligence (AI) can help further improve and automate the lifecycle of smart infrastructures to eventually improve their reliability and drive down their deployment and operation costs. Finally, based on the usage of AI for web and social networks as well as our previous experience in AI for networks and cyber-physical systems, we provide decision guidelines for the adoption of AI.",
Software Engineering for Machine Learning: A Case Study,"Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components-models may be 'entangled' in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.","Artifical Intelligence, Data, Machine Learning, Process, Software Engineering"
Stepwise adoption of continuous delivery in model-driven engineering,"Continuous Delivery (CD) and, in general, Continuous Software Engineering (CSE) is becoming the norm. Still, current practices and available integration platforms are too code-oriented. They are not well adapted to work with other, non text-based, software artifacts typically produced during early phases of the software engineering life-cycle. This is especially problematic for teams adopting a Model-Driven Engineering (MDE) approach to software development where several (meta)models (and model transformations) are built and executed as part of the development process. Typically, (part of) the code is automatically generated from such models. Therefore, in a complete CD process, changes in a model should trigger changes on the generated code when appropriate. A step further would be to apply CD practices to the development of modeling artefacts themselves. Analogously to traditional CD, where the goal is to have the mainline codebase always in a deployable state, the aim would be to have the modeling infrastructure always ready to be used. Those models could be the final product themselves or an intermediate artifact in a complete CSE process as described above. Either way, a tighter integration between CD and MDE would benefit software practitioners by providing them with complete CSE, covering also analysis and design stages of the process.","Continuous delivery, Continuous evolution, Model-driven engineering"
Streamlining DevOps automation for Cloud applications using TOSCA as standardized metamodel,"DevOps as an emerging paradigm aims to tightly integrate developers with operations personnel. This enables fast and frequent releases in the sense of continuously delivering new iterations of a particular application. Users and customers of today's Web applications and mobile apps running in the Cloud expect fast feedback to problems and feature requests. Thus, it is a critical competitive advantage to be able to respond quickly. Besides cultural and organizational changes that are necessary to apply DevOps in practice, tooling is required to implement end-to-end automation of deployment processes. Automation is the key to efficient collaboration and tight integration between development and operations. The DevOps community is constantly pushing new approaches, tools, and open-source artifacts to implement such automated processes. However, as all these proprietary and heterogeneous DevOps automation approaches differ from each other, it is hard to integrate and combine them to deploy applications in the Cloud using an automated deployment process. In this paper we present a systematic classification of DevOps artifacts and show how different kinds of artifacts can be discovered and transformed toward TOSCA, which is an emerging standard. We present an integrated modeling and runtime framework to enable the seamless and interoperable integration of different approaches to model and deploy application topologies. The framework is implemented by an open-source, end-to-end toolchain. Moreover, we validate and evaluate the presented approach to show its practical feasibility based on a detailed case study, in particular considering the performance of the transformation toward TOSCA.","Cloud computing, Cloud standards, Deployment automation, DevOps, TOSCA, Transformation"
Supporting efficient test automation using lightweight MBT,"The Agile and DevOps transformation of software development practices enhances the need for increased automation of functional testing, especially for regression testing. This poses challenges both in the effort that needs to be devoted to the creation and maintenance of automated test scripts, and in their relevance (i.e. their alignment with business needs). Test automation is still difficult to implement and maintain and the return on investment comes late while projects tend to be short. In this context, we have experimented a lightweight model-based test automation approach to address both productivity and relevance challenges. It integrates test automation through a simple process and tool-chain experimented on large IT projects.",
Supporting micro-services deployment in a safer way: A static analysis and automated rewriting approach,"The SOA ecosystem has drastically evolved since its childhood in the early 2000s. From monolithic services, micro-services now cooperate together in ultra-large scale systems. In this context, there is a tremendous need to deploy frequently new services, or new version of existing services. Container-based technologies (e.g., Docker) emerged recently to tool such deployments, promoting a black-box reuse mechanism to support off-the-shelf deployments. Unfortunately, from the service deployment point of view, such form of black-box reuse prevent to ensure what is really shipped inside the container with the service to deploy. In this paper, we propose a formalism to model and statically analyze service deployment artifacts based on state of the art deployment platforms. The static analysis mechanism leverages the hierarchy of deployment descriptors to verify a given deployment, as well as rewrite it to automatically fix common errors. The approach is validated through the automation of the guidelines provided by the user community associated to the reference Docker engine, and the analysis of 20,000 real deployment descriptors (hosted on GitHub).","Container, Docker, Microservice, Static analysis"
Tailoring MLOps Techniques for Industry 5.0 Needs,"It is a very popular era for machine learning (ML) applications, and Industry5.0 aims to have AI as one of its key technologies. Still, only a few ML initiatives make it to a production-grade implementation, mostly due to lacking proper Continuous Integration and Delivery framework and MLOps practices. This is especially true for industrial use cases, where the trust and reliability of ML applications are mission-critical. Most of these applications fail during the final stage of the development lifecycle, i.e. acceptance testing and validation of the ML application, while being integrated into Cyber-Physical System of Systems (CPSoS). This paper explores the key requirements for deploying ML applications in industrial scenarios, emphasizing the critical role of Digital Twins, edge AI, and responsible-explainable AI techniques in ensuring efficient and responsible operations. Building upon previous models, this paper suggests two process models: (i) the Olympics model for MLOps-coupled CPS engineering and (ii) the MLOps engineering toolchain for industrial applications.",
Teaching agile model-driven engineering for cyber-physical systems,"Agile development methods, model-driven engineering, and cyber-physical systems are important topics in software engineering education. It is not obvious how to teach their combination while respecting individual challenges posed to students and educators. We have devised a software project class for teaching the agile MDE for CPS. The project class was held in three different semesters. In this paper, we report on the setup of our exploratory study and its goals for teaching. We base our evaluation and insights on interviews and questionnaires. Our results show the feasibility of combination of agile MDE for CPS but also the challenges this combination poses to students and educators.","case study, cyber-physical systems, model-driven engineering, teaching"
Test Case Prioritization using Transfer Learning in Continuous Integration Environments,"The continuous Integration (CI) process runs a large set of automated test cases to verify software builds. The testing phase in the CI systems has timing constraints to ensure software quality without significantly delaying the CI builds. Therefore, CI requires efficient testing techniques such as Test Case Prioritization (TCP) to run faulty test cases with priority. Recent research studies on TCP utilize different Machine Learning (ML) methods to adopt the dynamic and complex nature of CI. However, the performance of ML for TCP may decrease for a low volume of data and less failure rate, whereas using existing data with similar patterns from other domains can be valuable. We formulate this as a transfer learning (TL) problem. TL has proven to be beneficial for many real-world applications where source domains have plenty of data, but the target domains have a scarcity of it. Therefore, this research investigates leveraging the benefit of transfer learning for test case prioritization (TCP). However, only some industrial CI datasets are publicly available due to data privacy protection regulations. In such cases, model-based transfer learning is a potential solution to share knowledge among different projects without revealing data to other stakeholders. This paper applies TransBoost, a tree-kernel-based TL algorithm, to evaluate the TL approach for 24 study subjects and identify potential source datasets.","Continuous Integration (CI), Test Case Prioritization (TCP), Transfer Learning (TL)"
Testing robot controllers using constraint programming and continuous integration,"Context: Testing complex industrial robots (CIRs) requires testing several interacting control systems. This is challenging, especially for robots performing process-intensive tasks such as painting or gluing, since their dedicated process control systems can be loosely coupled with the robot's motion control. Objective: Current practices for validating CIRs involve manual test case design and execution. To reduce testing costs and improve quality assurance, a trend is to automate the generation of test cases. Our work aims to define a cost-effective automated testing technique to validate CIR control systems in an industrial context. Method: This paper reports on a methodology, developed at ABB Robotics in collaboration with SIMULA, for the fully automated testing of CIRs control systems. Our approach draws on continuous integration principles and well-established constraint-based testing techniques. It is based on a novel constraintbased model for automatically generating test sequences where test sequences are both generated and executed as part of a continuous integration process. Results: By performing a detailed analysis of experimental results over a simplified version of our constraint model, we determine the most appropriate parameterization of the operational version of the constraint model. This version is now being deployed at ABB Robotics's CIR testing facilities and used on a permanent basis. This paper presents the empirical results obtained when automatically generating test sequences for CIRs at ABB Robotics. In a real industrial setting, the results show that our methodology is not only able to detect reintroduced known faults, but also to spot completely new faults. Conclusion: Our empirical evaluation shows that constraint-based testing is appropriate for automatically generating test sequences for CIRs and can be faithfully deployed in an industrial context.","Agile development, Constraint programming, Continuous integration, Distributed real time systems, Robotized painting, Software testing"
Testing self-healing cyber-physical systems under uncertainty with reinforcement learning: an empirical study,"The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.Self-healing is becoming an essential feature of Cyber-Physical Systems (CPSs). CPSs with this feature are named Self-Healing CPSs (SH-CPSs). SH-CPSs detect and recover from errors caused by hardware or software faults at runtime and handle uncertainties arising from their interactions with environments. Therefore, it is critical to test if SH-CPSs can still behave as expected under uncertainties. By testing an SH-CPS in various conditions and learning from testing results, reinforcement learning algorithms can gradually optimize their testing policies and apply the policies to detect failures, i.e., cases that the SH-CPS fails to behave as expected. However, there is insufficient evidence to know which reinforcement learning algorithms perform the best in terms of testing SH-CPSs behaviors including their self-healing behaviors under uncertainties. To this end, we conducted an empirical study to evaluate the performance of 14 combinations of reinforcement learning algorithms, with two value function learning based methods for operation invocations and seven policy optimization based algorithms for introducing uncertainties. Experimental results reveal that the 14 combinations of the algorithms achieved similar coverage of system states and transitions, and the combination of Q-learning and Uncertainty Policy Optimization (UPO) detected the most failures among the 14 combinations. On average, the Q-Learning and UPO combination managed to discover two times more failures than the others. Meanwhile, the combination took 52% less time to find a failure. Regarding scalability, the time and space costs of the value function learning based methods grow, as the number of states and transitions of the system under test increases. In contrast, increasing the systems complexity has little impact on policy optimization based algorithms.","Cyber-physical systems, Empirical evaluation, Model execution, Reinforcement learning, Self-healing, Uncertainty"
Testing self-healing cyber-physical systems under uncertainty: a fragility-oriented approach,"As an essential feature of smart cyber-physical systems (CPSs), self-healing behaviors play a major role in maintaining the normality of CPSs in the presence of faults and uncertainties. It is important to test whether self-healing behaviors can correctly heal faults under uncertainties to ensure their reliability. However, the autonomy of self-healing behaviors and impact of uncertainties make it challenging to conduct such testing. To this end, we devise a fragility-oriented testing approach, which is comprised of two novel algorithms: fragility-oriented testing (FOT) and uncertainty policy optimization (UPO). The two algorithms utilize the fragility, obtained from test executions, to learn the optimal policies for invoking operations and introducing uncertainties, respectively, to effectively detect faults. We evaluated their performance by comparing them against a coverage-oriented testing (COT) algorithm and a random uncertainty generation method (R). The evaluation results showed that the fault detection ability of FOT+UPO was significantly higher than the ones of FOT+R, COT+UPO, and COT+R, in 73 out of 81 cases. In the 73 cases, FOT+UPO detected more than 70% of faults, while the others detected 17% of faults, at the most.","Cyber-physical systems, Model execution, Reinforcement learning, Self-healing, Uncertainty"
Testing uncertainty of cyber-physical systems in IoT cloud infrastructures: Combining model-driven engineering and elastic execution,"Today's cyber-physical systems (CPS) span IoT and cloud-based datacenter infrastructures, which are highly heterogeneous with various types of uncertainty. Thus, testing uncertainties in these CPS is a challenging and multidisciplinary activity. We need several tools for modeling, deployment, control, and analytics to test and evaluate uncertainties for different configurations of the same CPS. In this paper, we explain why using state-of-the art model-driven engineering (MDE) and model-based testing (MBT) tools is not adequate for testing uncertainties of CPS in IoT Cloud infrastructures. We discus how to combine them with techniques for elastic execution to dynamically provision both CPS under test and testing utilities to perform tests in various IoT Cloud infrastructures.","Cloud, Elasticity, IoT, MBT, MDE, Testing, Uncertainty"
TestIt: An Open-Source Scalable Long-Term Autonomy Testing Toolkit for ROS,This paper presents an open-source testing toolkit TestIt that is primarily developed for model-based testing of autonomous systems to improve long-term autonomy. The architecture and tools within this architecture are introduced. The main novelty of presented solution is the scalable multi-pipeline testing architecture that enables incorporation of multi-purpose testing tools including those used in state-of-the-art model-based testing. The usability of TestIt for software testing in autonomous navigation context is demonstrated using Uppaal timed automata model based testing and Uppaal-family tools such as model checker and test execution environments Uppaal TRON and DTRON.,"autonomous robotics, integration testing, model-based testing, robot operating system, simulation, timed automata"
The adaptation of test-driven software processes to industrial automation engineering,"Software components provide an increasing part of added value in automation systems and become more complex to construct and test. Test-driven development (TDD) of software systems has been successfully used for agile development of business software systems. Test cases guide the system implementation and can be executed automatically after software changes (continuous integration & build strategy). However, TDD processes need to be adapted to control automation systems engineering, where real-world systems are challenging to model and to test automatically. In this paper we introduce an adapted TDD process from the business software engineering domain to industrial automation engineering. We identify a set of UML models that enable the systematic derivation of test cases. Based on an initial empirical study we evaluate the adapted TDD process based on an industrial use case to identify strength and limitation of this approach. Major results of the study were that UML models enabled effective test case derivation in the study context. © 2010 IEEE.",
The Digital Twin as a Common Knowledge Base in DevOps to Support Continuous System Evolution,"There is an industry wide push for faster and more feature rich systems, also in the development of Cyber-Physical Systems (CPS). Therefore, the need for applying agile development practices in the model-based design of CPS is becoming more widespread. This is no easy feat, as CPS are inherently complex, and their model-based development is less suited for agile development. Model-based development does suit the concept of digital twin, that is, design models representing a system instance in operation. In this paper we present an approach where the digital twins of system instances serve as a common-knowledge base for the entire agile development cycle of the system when performing system updates. Doing so enables interesting possibilities, such as the identification and detection of system variants, which is beneficial for the verification and validation of the system update. It also brings along challenges, as the executable physics based digital twin is generally computationally expensive. In this paper we introduce this approach by means of a small example of a swiveling pick and place robotic arm. We also elaborate on related work, and open future challenges.","Cyber-physical system, DevOps, Digital twin"
The DYNABIC approach to resilience of critical infrastructures,"With increasing interdependencies and evolving threats, maintaining operational continuity in critical systems has become a significant challenge. This paper presents the DYNABIC (Dynamic business continuity of critical infrastructures on top of adaptive multi-level cybersecurity) approach as a comprehensive framework to enhance the resilience of critical infrastructures. The DYNABIC approach provides the resilience enhancement through dynamic adaptation, automated response, collaboration, risk assessment, and continuous improvement. By fostering a proactive and collaborative approach to resilience, the DYNABIC framework empowers critical infrastructure sectors to effectively mitigate disruptions and recover from incidents. The paper explores the key components and architecture of the DYNABIC approach and highlights its potential to strengthen the resilience of critical infrastructures using the concept of Digital Twins in the face of evolving threats and complex operating environments involving cascading effects.","Critical Infrastructure Protection, Cybersecurity, Digital Twin, SecDevOps"
The Interoperability Challenge: Building a Model-Driven Digital Thread Platform for CPS,"With the heterogeneity of the industry 4.0 world, and more generally of the Cyberphysical Systems realm, the quest towards a platform approach to solve the interoperability problem is front and centre to any system and system-of-systems project. Traditional approaches cover individual aspects, like data exchange formats and published interfaces. They may adhere to some standard, however they hardly cover the production of the integration layer, which is implemented as bespoke glue code that is hard to produce and even harder to maintain. Therefore, the traditional integration approach often leads to poor code quality, further increasing the time and cost and reducing the agility, and a high reliance on the individual development skills. We are instead tackling the interoperability challenge by building a model driven/low-code Digital Thread platform that 1) systematizes the integration methodology, 2) provides methods and techniques for the individual integrations based on a layered Domain Specific Languages (DSL) approach, 3) through the DSLs it covers the integration space domain by domain, technology by technology, and is thus highly generalizable and reusable, 4) showcases a first collection of examples from the domains of robotics, IoT, data analytics, AI/ML and web applications, 5) brings cohesiveness to the aforementioned heterogeneous platform, and 6) is easier to understand and maintain, even by not specialized programmers. We showcase the power, versatility and the potential of the Digital Thread platform on four interoperability case studies: the generic extension to REST services, to robotics through the UR family of robots, to the integration of various external databases (for data integration) and to the provision of data analytics capabilities in R.","Digital Thread (DT), Interoperability, Low Code Development (LCD), Model Driven Development (MDD), Software platforms"
The next evolution of MDE: a seamless integration of machine learning into domain modeling,"Machine learning algorithms are designed to resolve unknown behaviors by extracting commonalities over massive datasets. Unfortunately, learning such global behaviors can be inaccurate and slow for systems composed of heterogeneous elements, which behave very differently, for instance as it is the case for cyber-physical systems and Internet of Things applications. Instead, to make smart decisions, such systems have to continuously refine the behavior on a per-element basis and compose these small learning units together. However, combining and composing learned behaviors from different elements is challenging and requires domain knowledge. Therefore, there is a need to structure and combine the learned behaviors and domain knowledge together in a flexible way. In this paper we propose to weave machine learning into domain modeling. More specifically, we suggest to decompose machine learning into reusable, chainable, and independently computable small learning units, which we refer to as microlearning units. These microlearning units are modeled together with and at the same level as the domain data. We show, based on a smart grid case study, that our approach can be significantly more accurate than learning a global behavior, while the performance is fast enough to be used for live learning.","Cyber-physical systems, Domain modeling, Live learning, Metamodeling, Model-driven engineering, Smart grids"
The systems engineering DevOps lemniscate and model-based system operations,"Systems engineering is defined as a full life cycle discipline and provides methodologies and processes to support the design, development, verification, sustainment, and disposal of systems. While this cradle-to-grave concept is well documented throughout literature, there has been recent emphasis on evolving and digitally transforming systems engineering methodologies, practices, and tools for the latter phases of system life cycles. This article adapts principles from the software engineering domain DevOps concept (a collaborative merger of system development and operations) into a Systems Engineering DevOps (SEDevOps) life cycle model. This facilitates a merger of systems engineering processes, tools, and products into a surrogate operational environment in which the sustainment of a system is tied closely to the curation of a system model expanded to include the enabling system elements necessary for operations and sustainment (procedures, scripts, etc.). This progression of the systems engineering mindset, focused on digitally transforming and enhancing system operations and sustainment, improves agility in later life cycle phases. A framework for applying SEDevOps is introduced as a new systems modeling language profile. A use-case leveraging this model-based system operations framework, shows how merging support elements into a spacecraft system model improves adaptability during operations, exemplifying elements of a DevOps approach to cyber-physical system sustainment.","Agile, DevOps, digital transformation, life cycle, model-based systems engineering (MBSE), systems engineering"
TOSCA4QC: Two Modeling Styles for TOSCA to Automate the Deployment and Orchestration of Quantum Applications,"Quantum computing introduces a new computing paradigm that promises to solve problems that cannot be solved by classical computers efficiently. Thus, quantum applications will be more and more integrated in classical applications. To bring these composite applications into production, technologies for an automated deployment and orchestration are required to avoid manual error-prone and time-consuming processes. For non-quantum applications, a variety of deployment technologies have been developed in recent years. However, the deployment of quantum applications currently differs significantly from non-quantum applications and thus, leads to a different modeling procedure for the deployment of quantum applications. To overcome these problems, we propose TOSCA4QC that introduces two deployment modeling styles based on the Topology and Orchestration Specification for Cloud Applications (TOSCA) standard for automating the deployment and orchestration of quantum applications: (i) SDK-specific modeling style to cover all technical deployment details and (ii) SDK-Agnostic modeling style supporting common modeling principles. We further show how existing model-driven development (MDD) approach can be applied to refine a SDK-Agnostic model to an executable SDK-specific model. We demonstrate the practical feasibility by a prototypical implementation as an extension of the TOSCA ecosystem OpenTOSCA and three case studies with IBMQ and a quantum simulator.","Deployment Automation, Modeling, Orchestration, Quantum Computing, TOSCA"
Towards a Domain-Specific Language for Provisioning Multiple Cloud Testing Environments for Mobile Applications,"Towards a Domain-Specific LanguagTowards a Domain-Specific Languag Provisioning testing environments for mobile applications is one of the most significant challenges within the software industry. Due to this high complexity that exists when provisioning test environments within the multiple available cloud platforms, it is necessary to make a significant investment in human resources, like time and effort for the implementation and execution of testing. There is additional complexity: testing software in a single environment is no longer sufficient. Today's mobile industry is constantly growing, and execution environments tend to be always different; the hardware configuration is usually different and sometimes exceeds the software barrier. It is challenging to execute testing on each of the existing devices, as this requires a long task of human intervention. Today some platforms provide testing services in different environments. However, not all providers have the complete set of environments that one would like to have, and specific knowledge is mandatory for using each available tool. It is a task that requires expertise and time. This work seeks to mitigate the impact on time and the learning curve through a high-level tool developed using a model-oriented approach, thus reducing the time needed for setting up each required platform for organizations. As a solution, we propose a Domain-Specific Language for provisioning multiple cloud testing environments for mobile applications. The configuration of the environment is done with the Domain Specific Language to make the usage easier by the final user. The necessary code is generated through transformations to set up an environment on cloud platforms such as Amazon Web Services (AWS) and Google Cloud Platform (GCP). This usage of this platform results in fewer code lines written and less time learning about the specific knowledge for each platform.","cloud, DevOPS, DSL, mobile, testing"
Towards a Model-Based DevOps for Cyber-Physical Systems,"The emerging field of Cyber-Physical Systems (CPS) calls for new scenarios of the use of models. In particular, CPS require to support both the integration of physical and cyber parts in innovative complex systems or production chains, together with the management of the data gathered from the environment to drive dynamic reconfiguration at runtime or finding improved designs. In such a context, the engineering of CPS must rely on models to uniformly reason about various heterogeneous concerns all along the system life cycle. In the last decades, the use of models has been intensively investigated both at design time for driving the development of complex systems, and at runtime as a reasoning layer to support deployment, monitoring and runtime adaptations. However, the approaches remain mostly independent. With the advent of DevOps principles, the engineering of CPS would benefit from supporting a smooth continuum of models from design to runtime, and vice versa. In this vision paper, we introduce a vision for supporting model-based DevOps practices, and we infer the corresponding research roadmap for the modeling community to address this vision by discussing a CPS demonstrator.",
Towards an assessment grid for intelligent modeling assistance,"The ever-growing complexity of systems, the growing number of stakeholders, and the corresponding continuous emergence of new domain-specific modeling abstractions has led to significantly higher cognitive load on modelers. There is an urgent need to provide modelers with better, more Intelligent Modeling Assistants (IMAs). An important factor to consider is the ability to assess and compare, to learn from existing and inform future IMAs, while potentially combining them. Recently, a conceptual Reference Framework for Intelligent Modeling Assistance (RF-IMA) was proposed. RF-IMA defines the main required components and high-level properties of IMAs. In this paper, we present a detailed, level-wise definition for the properties of RF-IMA to enable a better understanding, comparison, and selection of existing and future IMAs. The proposed levels are a first step towards a comprehensive assessment grid for intelligent modeling assistance. For an initial validation of the proposed levels, we assess the existing landscape of intelligent modeling assistance and three future scenarios of intelligent modeling assistance against these levels.","Artificial intelligence, Assessment levels, Feedback, Integrated development environment, Intelligent modeling assistance, Model-based software engineering"
Towards Blended Modeling and Simulation of DevOps Processes: the Keptn Case Study,"DevOps and Model Driven Engineering (MDE) provide differently skilled IT stakeholders with methodologies and tools for organizing and automating continuous software engineering activities and using models as key engineering artifacts. JSON is a popular data format, and JSON Schema provides a general-purpose schema language for JSON. This paper presents our work in progress on blended modeling and scenario simulation of continuous delivery pipelines as executable JSON-based models. For this purpose, we show a case study based on Keptn, an open source tool for DevOps automation of cloud-native applications, and its language, Shipyard, a JSON-based process language for continuous delivery pipeline specification.","blended modeling, DevOps, MDE, simulation"
Towards Continuous Consistency Checking of DevOps Artefacts,"DevOps tools are often scattered over a multitude of technologies, and thus, their integration is a challenging endeavour. The existing DevOps integration platforms, e.g., Keptn, often employ a family of languages for this purpose. However, as we have learnt from UML, SysML, and many others, a family of languages requires inter-model constraints to be checked in order to guarantee a high consistency between the different artefacts.In this work-in-progress paper, we propose a Model-Driven Engineering (MDE) approach for the continuous consistency checking of DevOps artefacts. First, we explicitly represent each artefact as a model, second, we establish links across them to set a navigable network of model elements; and third, we enable MDE services on top of this network.We envision the possibility of using GitOps to pull the DevOps artefacts, executing services for checking consistency and performing model repairs, uploading the changes to the DevOps tools, and finally pushing the artefacts to Git, thus resulting in a continuous consistency checking process in practice.","consistency management, DevOps, MDE"
Towards continuous delivery for domain experts: Using MDE to integrate non-programmers into a software delivery pipeline,"Modern computed tomography (CT) scanners are complex, software-intensive systems whose correct functioning is governed by over 100 parameters which depend on the concrete hardware configurations and on the addressed clinical use-cases. To tame the intrinsic complexity of the parameters configurations, over the last four years, Siemens Healthineers (SHS) have been developing and deploying a set of domain specific languages and tooling based on Jetbrains' Meta-Programming System. In this paper, we report on the challenges and experiences we made while building two delivery pipelines. At meta-level, we built a continuous delivery pipeline such that new versions of our domain specific modeling tool can be deployed continuously based on the feedback of domain experts. At model-level we have integrated the developed domain-specific tool in the continuous delivery pipeline for the computed tomography software and thereby bring the Continuous Delivery mind-set with advantages and challenges to domain experts who are working traditionally 'outside' of the software development.","Continuous delivery, ""Jetbrains MPS"", Model-driven engineering"
Towards Continuous Modelling to Enable DevOps: A Preliminary Study with Practitioners,"Model-based methods and techniques continuously evolve to meet the increasing challenges of modern-day technical landscapes. Parallel to Model-based methods, other paradigms are similarly maturing and being integrated, and one such paradigm is DevOps. Model-based methods and DevOps are perceived to provide benefits when viewed in isolation. Recently, there has been an increased interest in matching the two paradigms, with various proposals and early adoption results. However, little focus is put on the practitioners' view. In this paper, we propose a methodology that aims to utilise Model-driven engineering and DevOps practices in conjunction. Together with the methodology, we present an early evaluation of it from a practitioner's perspective. In particular, we study a large and long-running student project aiming to build a solar vehicle, by presenting the current integration and potential future directions. In this paper we limit the observation to the development phase. Early feedback from the case study indicates significant benefits for several identified project pain points, and it's expected that more benefits will emerge when more advanced DevOps aspects are integrated with model-based methods, and the project matures.","DevOps, model-based engineering, practitioners, simulink"
Towards Digital Twin-enabled DevOps for CPS providing Architecture-Based Service Adaptation & Verification at Runtime,"Background: Industrial Product-Service Systems (IPSS) denote a service-oriented way of providing access to cyber-physical systems' (CPS) capabilities. The design of such systems bears high risk due to uncertainty in requirements related to service function and behavior, operation environments, and evolving customer needs. Such risks and uncertainties are well known in the IT sector, where DevOps principles ensure continuous system improvement through reliable and frequent delivery processes. A modular and service-oriented system architecture complements these processes to facilitate IT system adaptation and evolution.Objective: This work proposes a method to use and extend the Digital Twins (DTs) of IPSS assets for enabling the continuous optimization of CPS service delivery and the latter's adaptation to changing needs and environments. This reduces uncertainty during design and operations by assuring IPSS integrity and availability, especially for design and service adaptations at CPS runtime.Methodology: The method builds on transferring IT DevOps principles to DT-enabled CPS IPSS. The chosen design approach integrates, reuses, and aligns the DT processing and communication resources with DevOps requirements derived from literature.Results: We use these requirements to propose a DT-enabled self-Adaptive CPS model, which guides the realization of DT-enabled DevOps in CPS IPSS. We further propose detailed design models for operation-critical DTs that integrate CPS closed-loop control and architecture-based CPS adaptation. This integrated approach enables the implementation of A/B testing as a use case and central concept to enable CPS IPSS service adaptation and reconfiguration.Conclusion: The self-Adaptive CPS model and DT design concept have been validated in an evaluation environment for operation-critical CPS IPSS. The demonstrator achieved sub-millisecond cycle times during service A/B testing at runtime without causing CPS operation interferences and downtime.","CPS, Deployment, DevOps, Digital Twin, IPSS, Self-Adaptation"
Towards Integrating Data-Driven Requirements Engineering into the Software Development Process: A Vision Paper,"[Context and motivation] Modern software engineering processes have shifted from traditional upfront requirements engineering (RE) to a more continuous way of conducting RE, particularly including data-driven approaches. [Question/problem] However, current research on data-driven RE focuses more on leveraging certain techniques such as natural language processing or machine learning than on making the concept fit for facilitating its use in the entire software development process. [Principal ideas/results] In this paper, we propose a research agenda composed of six distinct research directions. These include a data-driven RE infrastructure, embracing data heterogeneity, context-aware adaptation, data analysis and decision support, privacy and confidentiality, and finally process integration. Each of these directions addresses challenges that impede the broader use of data-driven RE. [Contribution] For researchers, our research agenda provides topics relevant to investigate. For practitioners, overcoming the underlying challenges with the help of the proposed research will allow to adopt a data-driven RE approach and facilitate its seamless integration into modern software engineering. For users, the proposed research will enable the transparency, control, and security needed to trust software systems and software providers.","Data-driven requirements engineering, Feedback gathering, Model-driven Engineering, Requirements monitoring"
Towards model-based continuous deployment of secure IoT systems,"Software development and delivery of IoT systems would greatly benefit from DevOps as their requirements for reliability, quality, security and privacy are paramount. The ability to continuously evolve these systems to adapt to their environment is decisive to ensure and increase their trustworthiness (including security and privacy) and quality. In particular, there is a need for supporting the continuous deployment of secure IoT systems over IoT, Edge, and Cloud infrastructures. However, our recent survey shows a lack of specific support for deploying security and privacy mechanisms as part of the system. This position paper reports on an on-going extension of the modelling language and models@runtime implementation of the Generation and Deployment of Smart IoT Systems (GeneSIS) tool for supporting continuous deployment of IoT security and privacy mechanisms on the Edge. In particular, we present our early design of the extended version of GeneSIS with the new concepts of port, security capability, and privacy capability.","Deployment, DevOps, IoT, Model-Driven Engineering, Model@runtime"
Towards Modeling Framework for DevOps: Requirements Derived from Industry Use Case,"To succeed with the development, deployment, and operation of the new generation of complex systems, organizations need the agility to adapt to constantly evolving environments. In this context, DevOps has emerged as an evolution of the agile approaches. It focuses on optimizing the flow of activities involved in the creation of end-user value, from idea to deployed functionality and operating systems. However, in spite of its popularity, DevOps still lacks proper engineering frameworks to support continuous improvement. One of our key objectives is to contribute to the development of a DevOps engineering framework composed of process, methods, and tools. A core part of this framework relates to the modeling of the different aspects of the DevOps system. To better understand the requirements of modeling in a DevOps context, we focus on a Product Build use case provided by an industry partner.","DevOps, Modeling, Process"
Towards Modelling Acceptance Tests as a Support for Software Measurement,"The DevOps paradigm emphasizes the need for a measurable feedback loop, starting from requirements and going as far as deployment in an automated way. In this context, a modelling challenge is to leverage the existing requirement engineering approaches to support measurements. Unfortunately, measurement methods are slow and costly by definition, preventing precisely measured requirements from being used in the DevOps loop. As a result, developers have to deal with grossly estimated elements, e.g., using story points promoted by agile methods. Thus, it is not possible to provide better support for the development team. We envision taking advantage of the artifacts that already exist in a DevOps context to provide better support for requirements measurement, making it available in an automated context such as the DevOps one. This paper focuses on the automated analysis of acceptance tests (e.g., expressed using the Gherkin language) to support functional measurement automation in a DevOps context. This proposition is illustrated by a scenario coming from an industrial partner, supporting the identification of four research challenges to be tackled.","acceptance tests, measurements, modelling"
Towards scalable model views on heterogeneous model resources,"When engineering complex systems, models are used to represent various systems aspects. These models are often heterogeneous in terms of modeling language, provenance, number or scale. They can be notably managed by different persistence frameworks adapted to their nature. As a result, the information relevant to engineers is usually split into several interrelated models. To be useful in practice, these models need to be integrated together to provide global views over the system under study. Model view approaches have been proposed to tackle such an issue. They provide an unification mechanism to combine and query heterogeneous models in a transparent way. These views usually target specific engineering tasks such as system design, monitoring, evolution, etc. In our present context, the MegaM@Rt2 industrially-supported European initiative defines a set of large-scale use cases where model views can be beneficial for tracing runtime and design time data. However, existing model view solutions mostly rely on in-memory constructs and low-level modeling APIs that have not been designed to scale in the context of large models stored in different kinds of sources. This paper presents the current status of our work towards a general solution to efficiently support scalable model views on heterogeneous model resources. It describes our integration approach between model view and model persistence frameworks. This notably implies the refinement of the view framework for the construction of large views from multiple model storage solutions. This also requires to study how parts of queries can be computed on the contributing models rather than on the view. Our solution has been benchmarked on a practical large-scale use case from the MegaM@Rt2 project, implementing a runtime - design time feedback loop. The corresponding EMF-based tooling support and modeling resources are fully available online.","Database, Design Time, Modeling, Persistence, Runtime, Scalability, Views"
Towards Specification Completion for Systems with Emergent Behavior based on DevOps,"Software systems may experience multiple emergent behaviors during their operation time. These emergent system behaviors occur when system engineers develop their system under the closed-world assumption, but this assumption is not met during its operation. This means that system engineers work on the basis that they have complete knowledge of the system and its environment during its design, when the system specification that has been created is actually incomplete. In this paper, an observation of an emergent behavior is considered to be a solid proof that the system model specification is still incomplete. A conceptual framework is proposed to harness the emergent behavior and complete the system specification that is provided during the its design. The framework consists of two parts, system development and system operations. It is built on a model-driven approach in order to provide a clear definition of the emergent behavior and a concrete development scheme. The framework exploits the DevOps paradigm as a successful paradigm to achieve the ultimate goal of developing complete system models through the continuous specification completion based on the observed emergent behavior. The goal of this framework is to help develop high-quality and reliable emergent systems based on the specification derived from the emergent behavior that occurred at run time.","Complex systems, DevOps, For-mal methods, Software development process, Software evolution"
Towards transparent combination of model management execution strategies for low-code development platforms,"Low-code development platforms are taking an important place in the model-driven engineering ecosystem, raising new challenges, among which transparent efficiency or scalability. Indeed, the increasing size of models leads to difficulties in interacting with them efficiently. To tackle this scalability issue, some tools are built upon specific computational strategies exploiting reactivity, or parallelism. However, their performances may vary depending on the specific nature of their usage. Choosing the most suitable computational strategy for a given usage is a difficult task which should be automated. Besides, the most efficient solutions may be obtained by the use of several strategies at the same time. This paper motivates the need for a transparent multi-strategy execution mode for model-management operations. We present an overview of the different computational strategies used in the model-driven engineering ecosystem, and use a running example to introduce the benefits of mixing strategies for performing a single computation. This example helps us present our design ideas for a multi-strategy model-management system. The code-related and DevOps challenges that emerged from this analysis are also presented.","Low-code development, Model-driven engineering, Multi-strategy, OCL, Spark"
TwinOps - DevOps meets model-based engineering and digital twins for the engineering of CPS,"The engineering of Cyber-Physical Systems (CPS) requires a large set of expertise to capture the system requirements and to derive a correct solution. Model-based Engineering and DevOps aim to efficiently deliver software with increased quality. Model-based Engineering relies on models as first-class artifacts to analyze, simulate, and ultimately generate parts of a system. DevOps focuses on software engineering activities, from early development to integration, and then improvement through the monitoring of the system at run-time. We claim these can be efficiently combined to improve the engineering process of CPS. In this paper, we present TwinOps, a process that unifies Model-based Engineering, Digital Twins, and DevOps practice in a uniform workflow. TwinOps illustrates how to leverage several best practices in MBE and DevOps for the engineering Cyber-Physical systems. We illustrate our contribution using a Digital Twins case study to illustrate TwinOps benefits, combining AADL and Modelica models, and an IoT platform.",
Use of Digital Twins and Digital Threads for Subway Infrastructure Monitoring,"The article deals with the problem of using digital twins and digital threads in complex distributed cyber-physical systems with a high level of structural and functional dynamics. A three-level model of the life cycle of a complex cyber-physical system is proposed. At the upper level, the observed system is described in terms of a continuous architecture. At the middle level, the observed system is described in terms of an agile architecture. At the lower level, the observed system is described in terms of a multigraph, which allows describe both the observed system structure and behavior. In the case study the solution of the problem of monitoring the state of the subway infrastructure is considered. The proposed approach has shown its effectiveness and can be applied in other domains such as smart cities.","Digital thread, Digital twin, Event monitoring and visualization, Machine learning, Neural network algorithms, Smart city"
Using DevOps toolchains in Agile model-driven engineering,"For model-driven engineering (MDE) to become more Agile, the community needs to embrace development and operations (DevOps) practices. One of the core practices of DevOps is the use of pipelines to enable CI/CD to make teams more Agile and break down the barriers between development and operations with faster deployments. Current MDE tooling is not designed at its core to participate in DevOps pipelines. Consequently this makes the adoption of MDE in industry more difficult. In this article, we cover an industrial experience report describing how we enabled our pipelines using DevOps and MDE.","Agile, Ant, CI/CD, DevOps, Eclipse, EMF, MDE, Model-driven engineering"
Using Metamodels to Improve Model-Based Testing of Service Orchestrations,"Online model-based testing is one of the most suitable techniques to assess the proper behavior of service orchestrations. However, the diverse panorama in terms of modeling languages and test case generation tools is a limitation to widespread adoption. We advocate that the application of Model-Driven Engineering principles as meta-modeling and model transformation can cope with this problem, improving the interoperability of artifacts in the test case generation process, thus bringing benefits in case of agile development processes, where system and technology evolution is frequent. In this paper, we present our contribution to this idea, introducing i) a reference metamodel, which stores the business process behavior and the information to generate input models for testing tools, and ii) transformations from orchestration languages towards testing tools. The proposed approach is implemented in a testing framework and evaluated on a case study where multiple orchestrations are expressed in two languages. Also, the paper presents how test cases are appropriately generated and successfully executed, starting from an orchestration model as a consequence of successful transformations.","Meta-modeling, Model-Based Testing, Model-Driven Engineering, SOA"
VeriDevOps: Automated Protection and Prevention to Meet Security Requirements in DevOps,"Current software development practices are increasingly based on using both COTS and legacy components which make such systems prone to security vulnerabilities. The modern practice addressing ever changing conditions, DevOps, promotes frequent software deliveries, however, verification methods artifacts should be updated in a timely fashion to cope with the pace of the process. VeriDevOps, Horizon 2020 project, aims at providing a faster feedback loop for verifying the security requirements and other quality attributes of large scale cyber-physical systems. VeriDevOps focuses on optimizing the security verification activities, by automatically creating verifiable models directly from security requirements formulated in natural language, using these models to check security properties on design models and then generating artefacts such as, tests or monitors that can be used later in the DevOps process. The main drivers for these advances are: Natural Language Processing, a combined formal verification and model-based testing approach, and machine-learning-based security monitors. VeriDevOps is in its initial stage - the project started on 1.10.2020 and it will run for three years. In this paper we will present the major conceptual ideas behind the project approach as well as the organizational settings.","Cybersecurity, Design checking, Machine Learning, Model-Driven Engineering, Natural Language Processing, Prevention and Reaction, Requirement, Root cause analysis, Runtime Analysis, Security-by-design, Testing and validation"
Verifying big data topologies by-design: a semi-automated approach,"Big data architectures have been gaining momentum in recent years. For instance, Twitter uses stream processing frameworks like Apache Storm to analyse billions of tweets per minute and learn the trending topics. However, architectures that process big data involve many different components interconnected via semantically different connectors. Such complex architectures make possible refactoring of the applications a difficult task for software architects, as applications might be very different with respect to the initial designs. As an aid to designers and developers, we developed OSTIA (Ordinary Static Topology Inference Analysis) that allows detecting the occurrence of common anti-patterns across big data architectures and exploiting software verification techniques on the elicited architectural models. This paper illustrates OSTIA and evaluates its uses and benefits on three industrial-scale case-studies.","Big data architectures, Big data systems verification, Software design and analysis"
When DevOps meets meta-learning: A portfolio to rule them all,"The Machine Learning (ML) world is in constant evolution, as the amount of different algorithms in this context is evolving quickly. Until now, it is the responsibility of data scientists to create ad-hoc ML pipelines for each situation they encounter, gaining knowledge about the adequacy between their context and the chosen pipeline. Considering that it is not possible at a human scale to analyze the exponential number of potential pipelines, picking the right pipeline that combines the proper preprocessing and algorithms is a hard task that requires knowledge and experience. In front of the complexity of building a right ML pipeline, algorithm portfolios aim to drive algorithm selection, learning from the past in a continuous process. However, building a portfolio requires that (i) data scientists develop and test pipelines and (ii) portfolio maintainers ensure the quality of the portfolio and enrich it. The firsts are the developers, while the seconds are the operators. In this paper, we present a set of criteria to be respected, and propose a pipeline-based meta-model, to support a DevOps approach in the context of Machine Learning Pipelines. The exploitation of this meta-model, both as a graph and as a logical expression, serves to ensure continuity between Dev and Ops. We depict our proposition through the simplified study of two primary use cases, one with developer's point-of-view, the other with ops'.","DevOps, MachineLearning, Meta-Learning, Pipeline"
